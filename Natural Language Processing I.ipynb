{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (I)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK installation\n",
    " 1. Install NLTK package using: pip install nltk \n",
    " 2. Open your python editor (Jupyter Notebook, Spyder etc.) and type the following comands below. Select \"all packages\" to install data included in NLTK, including corpora and books. It may take a few minutes to download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into tokens, phrases, or segments\n",
    "   * Clean up tokens and annotate tokens\n",
    "   * Extract features from tokens for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words, phrases, or segments\n",
    "   * Remove stop words and filter tokens\n",
    "   * POS (part of speech) Tagging\n",
    "   * Normalization: Stemming, Lemmatization\n",
    "   * Named Entity Recognition (NER)\n",
    "   * Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    "   * Create document-to-term matrix (bag of words)\n",
    " - NLP packages: NLTK, Gensim, spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import re    # import re module\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`strange days' chronicles the last two days of 1999 in los angeles. \\n as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips. \\n he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him. \\n this film features good performances, impressive film-making technique and breath-taking crowd scenes. \\n director kathryn bigelow knows her stuff and does not hesitate to use it. \\n but as a whole, this is an unsatisfying movie. \\n the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \\n not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \\n the film just ends up preachy, unexciting and uninvolving.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2.1. Load the text for analysis\n",
    "\n",
    "text='''`strange days' chronicles the last two days of 1999 in los angeles. \n",
    " as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips. \n",
    " he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him. \n",
    " this film features good performances, impressive film-making technique and breath-taking crowd scenes. \n",
    " director kathryn bigelow knows her stuff and does not hesitate to use it. \n",
    " but as a whole, this is an unsatisfying movie. \n",
    " the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \n",
    " not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \n",
    " the film just ends up preachy, unexciting and uninvolving.'''\n",
    "\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Tokenization\n",
    " - **Definition**: the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "    * Sentence\n",
    " - Different methods exist:\n",
    "    * Split by regular expression patterns\n",
    "    * NLTK's word tokenizer\n",
    "    * NLTK's regular expression tokenizer (customizable)\n",
    " - None of them can be perfect for any tokenization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "['', 'strange', 'days', 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', 'lenny', 'nero', 'ralph', 'fiennes', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', 'he', 'pines', 'for', 'his', 'ex', 'girlfriend', 'faith', 'juliette', 'lewis', 'but', 'doesn', 't', 'notice', 'that', 'another', 'friend', 'mace', 'angela', 'bassett', 'really', 'cares', 'for', 'him', 'this', 'film', 'features', 'good', 'performances', 'impressive', 'film', 'making', 'technique', 'and', 'breath', 'taking', 'crowd', 'scenes', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', 'but', 'as', 'a', 'whole', 'this', 'is', 'an', 'unsatisfying', 'movie', 'the', 'problem', 'is', 'that', 'the', 'writers', 'james', 'cameron', 'and', 'jay', 'cocks', 'were', 'too', 'ambitious', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', 'thrills', 'and', 'drama', 'not', 'that', 'ambitious', 'film', 'making', 'should', 'be', 'discouraged', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', 'it', 'fails', 'badly', 'and', 'obviously', 'the', 'film', 'just', 'ends', 'up', 'preachy', 'unexciting', 'and', 'uninvolving', '']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.1. Simply split the text by one or more non-word characters\n",
    "\n",
    "# \\W+: one or more non-words\n",
    "tokens = re.split(r\"\\W+\", text)   \n",
    "\n",
    "# get the number of tokens\n",
    "\n",
    "print(len(tokens))                   \n",
    "print(tokens)                     \n",
    "\n",
    "# Pros: no punctuation, just words\n",
    "# Cons: breath-taking and film-making, doesn't\n",
    "# are split into two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "['`', 'strange', 'days', \"'\", 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', '.', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', ',', 'lenny', 'nero', '(', 'ralph', 'fiennes', ')', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', '.', 'he', 'pines', 'for', 'his', 'ex-girlfriend', ',', 'faith', '(', 'juliette', 'lewis', ')', ',', 'but', 'does', \"n't\", 'notice', 'that', 'another', 'friend', ',', 'mace', '(', 'angela', 'bassett', ')', 'really', 'cares', 'for', 'him', '.', 'this', 'film', 'features', 'good', 'performances', ',', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', '.', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', '.', 'but', 'as', 'a', 'whole', ',', 'this', 'is', 'an', 'unsatisfying', 'movie', '.', 'the', 'problem', 'is', 'that', 'the', 'writers', ',', 'james', 'cameron', 'and', 'jay', 'cocks', ',', 'were', 'too', 'ambitious', ',', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', ',', 'thrills', ',', 'and', 'drama', '.', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', ';', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', ',', 'it', 'fails', 'badly', 'and', 'obviously', '.', 'the', 'film', 'just', 'ends', 'up', 'preachy', ',', 'unexciting', 'and', 'uninvolving', '.']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.2 NLTK's word tokenizer: \n",
    "\n",
    "# break down text into words and punctuations\n",
    "\n",
    "# invoke NLTK's word tokenizer\n",
    "tokens = nltk.word_tokenize(text)    \n",
    "print(len(tokens) )                   \n",
    "print (tokens)       \n",
    "\n",
    "# Pros: words are well tokenized, \n",
    "# e.g. breath-taking and film-making each is captured as one word\n",
    "# doesn't becomes does n't\n",
    "# Pros: need to remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "['strange', 'days', 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', 'lenny', 'nero', 'ralph', 'fiennes', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', 'he', 'pines', 'for', 'his', 'ex-girlfriend', 'faith', 'juliette', 'lewis', 'but', 'does', \"n't\", 'notice', 'that', 'another', 'friend', 'mace', 'angela', 'bassett', 'really', 'cares', 'for', 'him', 'this', 'film', 'features', 'good', 'performances', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', 'but', 'as', 'a', 'whole', 'this', 'is', 'an', 'unsatisfying', 'movie', 'the', 'problem', 'is', 'that', 'the', 'writers', 'james', 'cameron', 'and', 'jay', 'cocks', 'were', 'too', 'ambitious', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', 'thrills', 'and', 'drama', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', 'it', 'fails', 'badly', 'and', 'obviously', 'the', 'film', 'just', 'ends', 'up', 'preachy', 'unexciting', 'and', 'uninvolving']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.3 remove leading or trailing punctuations\n",
    "\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "tokens=[token.strip(string.punctuation) for token in tokens]\n",
    "\n",
    "# remove empty tokens\n",
    "tokens=[token.strip() for token in tokens if token.strip()!='']\n",
    "print(len(tokens) )\n",
    "print(tokens)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n",
      "['strange', 'days', 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', 'lenny', 'nero', 'ralph', 'fiennes', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', 'he', 'pines', 'for', 'his', 'ex-girlfriend', 'faith', 'juliette', 'lewis', 'but', \"doesn't\", 'notice', 'that', 'another', 'friend', 'mace', 'angela', 'bassett', 'really', 'cares', 'for', 'him', 'this', 'film', 'features', 'good', 'performances', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', 'but', 'as', 'whole', 'this', 'is', 'an', 'unsatisfying', 'movie', 'the', 'problem', 'is', 'that', 'the', 'writers', 'james', 'cameron', 'and', 'jay', 'cocks', 'were', 'too', 'ambitious', 'aiming', 'for', 'film', 'with', 'social', 'relevance', 'thrills', 'and', 'drama', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', 'it', 'fails', 'badly', 'and', 'obviously', 'the', 'film', 'just', 'ends', 'up', 'preachy', 'unexciting', 'and', 'uninvolving']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.4 NLTK's regular expression tokenizer (customizable)\n",
    "\n",
    "# Pattern can be customized to your need\n",
    "\n",
    "# a word is defined as:\n",
    "# (1) must start with a word character  \n",
    "# (2) then contain zero or more word characters,\"-\", \n",
    "#     or \"'\" in the middle \n",
    "# (3) must end with a word character\n",
    "# e.g. film-making, doesn't\n",
    "\n",
    "pattern=r'\\w[\\w\\'-]*\\w'                        \n",
    "\n",
    "# call NLTK's regular expression tokenization\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "print(len(tokens))\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[\"strange days' chronicles the last two days of 1999 in los angeles.\", 'as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips.', \"he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him.\", 'this film features good performances, impressive film-making technique and breath-taking crowd scenes.', 'director kathryn bigelow knows her stuff and does not hesitate to use it.', 'but as a whole, this is an unsatisfying movie.', 'the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama.', 'not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously.', 'the film just ends up preachy, unexciting and uninvolving.']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.5 Use NLTK's regular expression tokenizer \n",
    "# to define sentences, i.e. \n",
    "# (1) starts with non-space character, \n",
    "# (2) contains any number of characters in the middle, \n",
    "#     as long as they are not \"!?.\"\n",
    "# (3) ends with !?.\n",
    "\n",
    "pattern=r'\\w[^!?.]*[?!.]'  \n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "print(len(tokens))\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"`strange days' chronicles the last two days of 1999 in los angeles.\",\n",
       " 'as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips.',\n",
       " \"he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him.\",\n",
       " 'this film features good performances, impressive film-making technique and breath-taking crowd scenes.',\n",
       " 'director kathryn bigelow knows her stuff and does not hesitate to use it.',\n",
       " 'but as a whole, this is an unsatisfying movie.',\n",
       " 'the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama.',\n",
       " 'not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously.',\n",
       " 'the film just ends up preachy, unexciting and uninvolving.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.2.1. Segmentation by Sentences #分割\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)\n",
    "sentences\n",
    "\n",
    "# what patterns can be used to segment \n",
    "# text into sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Phrases: Bigrams (2 consecutive words),  Trigrams (3 consecutive words), or in general n-grams\n",
    " - Why bigrams and trigrams?\n",
    " - How to get bigrams or trigrams:\n",
    "    1. First tokenize text into unigrams\n",
    "    2. Slice through the list of unigrams to get bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('`', 'strange'), ('strange', 'days'), ('days', \"'\"), (\"'\", 'chronicles'), ('chronicles', 'the'), ('the', 'last'), ('last', 'two'), ('two', 'days'), ('days', 'of'), ('of', '1999'), ('1999', 'in'), ('in', 'los'), ('los', 'angeles'), ('angeles', '.'), ('.', 'as'), ('as', 'the'), ('the', 'locals'), ('locals', 'gear'), ('gear', 'up'), ('up', 'for'), ('for', 'the'), ('the', 'new'), ('new', 'millenium'), ('millenium', ','), (',', 'lenny'), ('lenny', 'nero'), ('nero', '('), ('(', 'ralph'), ('ralph', 'fiennes'), ('fiennes', ')'), (')', 'goes'), ('goes', 'about'), ('about', 'his'), ('his', 'business'), ('business', 'of'), ('of', 'peddling'), ('peddling', 'erotic'), ('erotic', 'memory'), ('memory', 'clips'), ('clips', '.'), ('.', 'he'), ('he', 'pines'), ('pines', 'for'), ('for', 'his'), ('his', 'ex-girlfriend'), ('ex-girlfriend', ','), (',', 'faith'), ('faith', '('), ('(', 'juliette'), ('juliette', 'lewis'), ('lewis', ')'), (')', ','), (',', 'but'), ('but', 'does'), ('does', \"n't\"), (\"n't\", 'notice'), ('notice', 'that'), ('that', 'another'), ('another', 'friend'), ('friend', ','), (',', 'mace'), ('mace', '('), ('(', 'angela'), ('angela', 'bassett'), ('bassett', ')'), (')', 'really'), ('really', 'cares'), ('cares', 'for'), ('for', 'him'), ('him', '.'), ('.', 'this'), ('this', 'film'), ('film', 'features'), ('features', 'good'), ('good', 'performances'), ('performances', ','), (',', 'impressive'), ('impressive', 'film-making'), ('film-making', 'technique'), ('technique', 'and'), ('and', 'breath-taking'), ('breath-taking', 'crowd'), ('crowd', 'scenes'), ('scenes', '.'), ('.', 'director'), ('director', 'kathryn'), ('kathryn', 'bigelow'), ('bigelow', 'knows'), ('knows', 'her'), ('her', 'stuff'), ('stuff', 'and'), ('and', 'does'), ('does', 'not'), ('not', 'hesitate'), ('hesitate', 'to'), ('to', 'use'), ('use', 'it'), ('it', '.'), ('.', 'but'), ('but', 'as'), ('as', 'a'), ('a', 'whole'), ('whole', ','), (',', 'this'), ('this', 'is'), ('is', 'an'), ('an', 'unsatisfying'), ('unsatisfying', 'movie'), ('movie', '.'), ('.', 'the'), ('the', 'problem'), ('problem', 'is'), ('is', 'that'), ('that', 'the'), ('the', 'writers'), ('writers', ','), (',', 'james'), ('james', 'cameron'), ('cameron', 'and'), ('and', 'jay'), ('jay', 'cocks'), ('cocks', ','), (',', 'were'), ('were', 'too'), ('too', 'ambitious'), ('ambitious', ','), (',', 'aiming'), ('aiming', 'for'), ('for', 'a'), ('a', 'film'), ('film', 'with'), ('with', 'social'), ('social', 'relevance'), ('relevance', ','), (',', 'thrills'), ('thrills', ','), (',', 'and'), ('and', 'drama'), ('drama', '.'), ('.', 'not'), ('not', 'that'), ('that', 'ambitious'), ('ambitious', 'film-making'), ('film-making', 'should'), ('should', 'be'), ('be', 'discouraged'), ('discouraged', ';'), (';', 'just'), ('just', 'that'), ('that', 'when'), ('when', 'it'), ('it', 'fails'), ('fails', 'to'), ('to', 'achieve'), ('achieve', 'its'), ('its', 'goals'), ('goals', ','), (',', 'it'), ('it', 'fails'), ('fails', 'badly'), ('badly', 'and'), ('and', 'obviously'), ('obviously', '.'), ('.', 'the'), ('the', 'film'), ('film', 'just'), ('just', 'ends'), ('ends', 'up'), ('up', 'preachy'), ('preachy', ','), (',', 'unexciting'), ('unexciting', 'and'), ('and', 'uninvolving'), ('uninvolving', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('`', 'strange', 'days'),\n",
       " ('strange', 'days', \"'\"),\n",
       " ('days', \"'\", 'chronicles'),\n",
       " (\"'\", 'chronicles', 'the'),\n",
       " ('chronicles', 'the', 'last'),\n",
       " ('the', 'last', 'two'),\n",
       " ('last', 'two', 'days'),\n",
       " ('two', 'days', 'of'),\n",
       " ('days', 'of', '1999'),\n",
       " ('of', '1999', 'in'),\n",
       " ('1999', 'in', 'los'),\n",
       " ('in', 'los', 'angeles'),\n",
       " ('los', 'angeles', '.'),\n",
       " ('angeles', '.', 'as'),\n",
       " ('.', 'as', 'the'),\n",
       " ('as', 'the', 'locals'),\n",
       " ('the', 'locals', 'gear'),\n",
       " ('locals', 'gear', 'up'),\n",
       " ('gear', 'up', 'for'),\n",
       " ('up', 'for', 'the'),\n",
       " ('for', 'the', 'new'),\n",
       " ('the', 'new', 'millenium'),\n",
       " ('new', 'millenium', ','),\n",
       " ('millenium', ',', 'lenny'),\n",
       " (',', 'lenny', 'nero'),\n",
       " ('lenny', 'nero', '('),\n",
       " ('nero', '(', 'ralph'),\n",
       " ('(', 'ralph', 'fiennes'),\n",
       " ('ralph', 'fiennes', ')'),\n",
       " ('fiennes', ')', 'goes'),\n",
       " (')', 'goes', 'about'),\n",
       " ('goes', 'about', 'his'),\n",
       " ('about', 'his', 'business'),\n",
       " ('his', 'business', 'of'),\n",
       " ('business', 'of', 'peddling'),\n",
       " ('of', 'peddling', 'erotic'),\n",
       " ('peddling', 'erotic', 'memory'),\n",
       " ('erotic', 'memory', 'clips'),\n",
       " ('memory', 'clips', '.'),\n",
       " ('clips', '.', 'he'),\n",
       " ('.', 'he', 'pines'),\n",
       " ('he', 'pines', 'for'),\n",
       " ('pines', 'for', 'his'),\n",
       " ('for', 'his', 'ex-girlfriend'),\n",
       " ('his', 'ex-girlfriend', ','),\n",
       " ('ex-girlfriend', ',', 'faith'),\n",
       " (',', 'faith', '('),\n",
       " ('faith', '(', 'juliette'),\n",
       " ('(', 'juliette', 'lewis'),\n",
       " ('juliette', 'lewis', ')'),\n",
       " ('lewis', ')', ','),\n",
       " (')', ',', 'but'),\n",
       " (',', 'but', 'does'),\n",
       " ('but', 'does', \"n't\"),\n",
       " ('does', \"n't\", 'notice'),\n",
       " (\"n't\", 'notice', 'that'),\n",
       " ('notice', 'that', 'another'),\n",
       " ('that', 'another', 'friend'),\n",
       " ('another', 'friend', ','),\n",
       " ('friend', ',', 'mace'),\n",
       " (',', 'mace', '('),\n",
       " ('mace', '(', 'angela'),\n",
       " ('(', 'angela', 'bassett'),\n",
       " ('angela', 'bassett', ')'),\n",
       " ('bassett', ')', 'really'),\n",
       " (')', 'really', 'cares'),\n",
       " ('really', 'cares', 'for'),\n",
       " ('cares', 'for', 'him'),\n",
       " ('for', 'him', '.'),\n",
       " ('him', '.', 'this'),\n",
       " ('.', 'this', 'film'),\n",
       " ('this', 'film', 'features'),\n",
       " ('film', 'features', 'good'),\n",
       " ('features', 'good', 'performances'),\n",
       " ('good', 'performances', ','),\n",
       " ('performances', ',', 'impressive'),\n",
       " (',', 'impressive', 'film-making'),\n",
       " ('impressive', 'film-making', 'technique'),\n",
       " ('film-making', 'technique', 'and'),\n",
       " ('technique', 'and', 'breath-taking'),\n",
       " ('and', 'breath-taking', 'crowd'),\n",
       " ('breath-taking', 'crowd', 'scenes'),\n",
       " ('crowd', 'scenes', '.'),\n",
       " ('scenes', '.', 'director'),\n",
       " ('.', 'director', 'kathryn'),\n",
       " ('director', 'kathryn', 'bigelow'),\n",
       " ('kathryn', 'bigelow', 'knows'),\n",
       " ('bigelow', 'knows', 'her'),\n",
       " ('knows', 'her', 'stuff'),\n",
       " ('her', 'stuff', 'and'),\n",
       " ('stuff', 'and', 'does'),\n",
       " ('and', 'does', 'not'),\n",
       " ('does', 'not', 'hesitate'),\n",
       " ('not', 'hesitate', 'to'),\n",
       " ('hesitate', 'to', 'use'),\n",
       " ('to', 'use', 'it'),\n",
       " ('use', 'it', '.'),\n",
       " ('it', '.', 'but'),\n",
       " ('.', 'but', 'as'),\n",
       " ('but', 'as', 'a'),\n",
       " ('as', 'a', 'whole'),\n",
       " ('a', 'whole', ','),\n",
       " ('whole', ',', 'this'),\n",
       " (',', 'this', 'is'),\n",
       " ('this', 'is', 'an'),\n",
       " ('is', 'an', 'unsatisfying'),\n",
       " ('an', 'unsatisfying', 'movie'),\n",
       " ('unsatisfying', 'movie', '.'),\n",
       " ('movie', '.', 'the'),\n",
       " ('.', 'the', 'problem'),\n",
       " ('the', 'problem', 'is'),\n",
       " ('problem', 'is', 'that'),\n",
       " ('is', 'that', 'the'),\n",
       " ('that', 'the', 'writers'),\n",
       " ('the', 'writers', ','),\n",
       " ('writers', ',', 'james'),\n",
       " (',', 'james', 'cameron'),\n",
       " ('james', 'cameron', 'and'),\n",
       " ('cameron', 'and', 'jay'),\n",
       " ('and', 'jay', 'cocks'),\n",
       " ('jay', 'cocks', ','),\n",
       " ('cocks', ',', 'were'),\n",
       " (',', 'were', 'too'),\n",
       " ('were', 'too', 'ambitious'),\n",
       " ('too', 'ambitious', ','),\n",
       " ('ambitious', ',', 'aiming'),\n",
       " (',', 'aiming', 'for'),\n",
       " ('aiming', 'for', 'a'),\n",
       " ('for', 'a', 'film'),\n",
       " ('a', 'film', 'with'),\n",
       " ('film', 'with', 'social'),\n",
       " ('with', 'social', 'relevance'),\n",
       " ('social', 'relevance', ','),\n",
       " ('relevance', ',', 'thrills'),\n",
       " (',', 'thrills', ','),\n",
       " ('thrills', ',', 'and'),\n",
       " (',', 'and', 'drama'),\n",
       " ('and', 'drama', '.'),\n",
       " ('drama', '.', 'not'),\n",
       " ('.', 'not', 'that'),\n",
       " ('not', 'that', 'ambitious'),\n",
       " ('that', 'ambitious', 'film-making'),\n",
       " ('ambitious', 'film-making', 'should'),\n",
       " ('film-making', 'should', 'be'),\n",
       " ('should', 'be', 'discouraged'),\n",
       " ('be', 'discouraged', ';'),\n",
       " ('discouraged', ';', 'just'),\n",
       " (';', 'just', 'that'),\n",
       " ('just', 'that', 'when'),\n",
       " ('that', 'when', 'it'),\n",
       " ('when', 'it', 'fails'),\n",
       " ('it', 'fails', 'to'),\n",
       " ('fails', 'to', 'achieve'),\n",
       " ('to', 'achieve', 'its'),\n",
       " ('achieve', 'its', 'goals'),\n",
       " ('its', 'goals', ','),\n",
       " ('goals', ',', 'it'),\n",
       " (',', 'it', 'fails'),\n",
       " ('it', 'fails', 'badly'),\n",
       " ('fails', 'badly', 'and'),\n",
       " ('badly', 'and', 'obviously'),\n",
       " ('and', 'obviously', '.'),\n",
       " ('obviously', '.', 'the'),\n",
       " ('.', 'the', 'film'),\n",
       " ('the', 'film', 'just'),\n",
       " ('film', 'just', 'ends'),\n",
       " ('just', 'ends', 'up'),\n",
       " ('ends', 'up', 'preachy'),\n",
       " ('up', 'preachy', ','),\n",
       " ('preachy', ',', 'unexciting'),\n",
       " (',', 'unexciting', 'and'),\n",
       " ('unexciting', 'and', 'uninvolving'),\n",
       " ('and', 'uninvolving', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.3.1. Get bigrams from the text                       \n",
    "\n",
    "# bigrams are formed from unigrams\n",
    "# nltk.bigram returns an iterator\n",
    "tokens = nltk.word_tokenize(text)\n",
    "bigrams=list(nltk.bigrams(tokens))  # tokens are created in Exercise 3.1.4\n",
    "print(bigrams)\n",
    "\n",
    "# trigrams\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Collocation\n",
    " - Most bigrams or trigrams may sound odd. However, we need to pay attention to frequent bigrams or trigrams\n",
    " - **Collocation**: an expression consisting of two or more words that correspond to some conventional way of saying things, e.g. red wine, United States, graduate students etc.\n",
    "    - Collocations are not fully compositional in that there is usually an element of meaning added to the combination.\n",
    " - Question: how to find collocations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 'the'),\n",
       " ('it', 'fails'),\n",
       " (\"'\", 'chronicles'),\n",
       " ('(', 'angela'),\n",
       " ('(', 'juliette'),\n",
       " ('(', 'ralph'),\n",
       " (')', ','),\n",
       " (')', 'goes'),\n",
       " (')', 'really'),\n",
       " (',', 'aiming')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.1. Get collocation\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# bigram association measures\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# construct bigrams using words from our example\n",
    "finder = BigramCollocationFinder.from_words(tokens) # tokens are created in Exercise 3.1.4\n",
    "\n",
    "# the corpus is too small\n",
    "finder.nbest(bigram_measures.raw_freq, 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'and'),\n",
       " (',', '\"'),\n",
       " ('of', 'the'),\n",
       " (\"'\", 's'),\n",
       " ('in', 'the'),\n",
       " ('said', ','),\n",
       " ('said', 'to'),\n",
       " ('.', 'He'),\n",
       " ('the', 'land'),\n",
       " ('.', 'The')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct bigrams using words from a large bulit-in NLTK corpus\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(\\\n",
    "        nltk.corpus.genesis.words('english-web.txt'))\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10) \n",
    "\n",
    "# Note that the most frequent bigrams are very odd\n",
    "# how to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('God', 'said'),\n",
       " ('one', 'hundred'),\n",
       " ('Jacob', 'said'),\n",
       " ('Yahweh', 'God'),\n",
       " ('Yahweh', 'said'),\n",
       " ('years', 'old'),\n",
       " ('seven', 'years'),\n",
       " ('Joseph', 'said'),\n",
       " ('every', 'man'),\n",
       " ('five', 'years')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.2. Find collocation by filter\n",
    "\n",
    "import string\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words\\\n",
    "                         or w.strip(string.punctuation)=='')\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10) \n",
    "\n",
    "# better?\n",
    "# most of them are in the pattern of \"xxx said\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 How to find collocations - PMI\n",
    "- By **frequency** (perhaps with filter)\n",
    "- **Pointwise Mutual Information (PMI)**\n",
    "  - giving two words $w_1, w_2$, $$PMI(w_1,w_2)=\\log{\\frac{p(w_1,w_2)}{p(w_1)*p(w_2)}}$$\n",
    "  - Some observations:\n",
    "    - if $w_1$ and $w_2$ are independent, $PMI(w_1,w_2)=0$\n",
    "    - if $w_1$ is completely dependent on $w_2$, i.e. $p(w_1,w_2)=p(w_2)$, $PMI(w_1,w_2)=\\frac{1}{p(w_1)}$. In this case, what if $w_1$ just appear once in the corpus? \n",
    "    - PMI favors less frequent collocations \n",
    "    - how to fix it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Allon', 'Bacuth'),\n",
       " ('Ashteroth', 'Karnaim'),\n",
       " ('Ben', 'Ammi'),\n",
       " ('En', 'Mishpat'),\n",
       " ('Jegar', 'Sahadutha'),\n",
       " ('Salt', 'Sea'),\n",
       " ('Whoever', 'sheds'),\n",
       " ('appoint', 'overseers'),\n",
       " ('aromatic', 'resin'),\n",
       " ('cutting', 'instrument')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.1.1 Metrics for Collocations\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# load a built-in NLTK corpus as a list of words\n",
    "words=nltk.corpus.genesis.words('english-web.txt')\n",
    "\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# find top-n bigrams by pmi\n",
    "finder.nbest(bigram_measures.pmi, 10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('burnt', 'offering'),\n",
       " ('Paddan', 'Aram'),\n",
       " ('living', 'creature'),\n",
       " ('young', 'lady'),\n",
       " ('little', 'ones'),\n",
       " ('Be', 'fruitful'),\n",
       " ('still', 'alive'),\n",
       " ('savory', 'food'),\n",
       " ('creeping', 'thing'),\n",
       " ('find', 'favor')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4.1.2 filter bigrams by frequency\n",
    "# only trigrams that appear 5+ times\n",
    "finder.apply_freq_filter(5)\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 How to find collocations - NPMI and others\n",
    "- **Normalized Pointwise Mutual Information (NPMI)**\n",
    "   - If $w_1$ and $w_2$ always occur together, i.e., $p(w_1)=p(w_2)=p(w_1,w_2)$, PMI reaches the maximum: $$PMI(w_1,w_2)=-\\log{p(w_1)}=-\\log{p(w_2)}=-\\log{p(w_1,w_2)}$$\n",
    "   - Normalized PMI is the PMI divided by the upper bound:\n",
    "   $$PMI(w_1,w_2)=\\frac{\\log{\\frac{p(w_1,w_2)}{(p(w_1)*p(w_2))}}}{-\\log{p(w_1,w_2)}}$$\n",
    "   \n",
    "- Another simple method by Mikolov et al. (2013) (https://arxiv.org/pdf/1310.4546.pdf):\n",
    "\n",
    "$$Score(w_1, w_2)=\\frac{count(w_1,w_2)-\\delta}{count(w_1)*count(w_2)}$$, where $\\delta$ is the minimum collocation frequency\n",
    "- Both methods are implemented in gensim package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.5. Vocabulary \n",
    " - Vocabulary: the set of unique tokens (unigrams/phrases)  \n",
    " - Dictionary: typicallly, the vocabulary of a text can be represented as a dictionary \n",
    "    * Key: word, Value: count of the word\n",
    "    * **nltk.FreqDist()**: a nice function for calculating frequncy of words/phrases\n",
    "        - Get the frequency of items in the parameter list \n",
    "        - Retruns an object similar to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dist: <FreqDist with 115 samples and 175 outcomes>\n",
      "top 10 words: [(',', 13), ('.', 9), ('the', 6), ('and', 6), ('for', 4), ('that', 4), ('(', 3), (')', 3), ('film', 3), ('it', 3)]\n",
      "` : 1\n",
      "strange : 1\n",
      "days : 2\n",
      "' : 1\n",
      "chronicles : 1\n",
      "the : 6\n",
      "last : 1\n",
      "two : 1\n",
      "of : 2\n",
      "1999 : 1\n",
      "in : 1\n",
      "los : 1\n",
      "angeles : 1\n",
      ". : 9\n",
      "as : 2\n",
      "locals : 1\n",
      "gear : 1\n",
      "up : 2\n",
      "for : 4\n",
      "new : 1\n",
      "millenium : 1\n",
      ", : 13\n",
      "lenny : 1\n",
      "nero : 1\n",
      "( : 3\n",
      "ralph : 1\n",
      "fiennes : 1\n",
      ") : 3\n",
      "goes : 1\n",
      "about : 1\n",
      "his : 2\n",
      "business : 1\n",
      "peddling : 1\n",
      "erotic : 1\n",
      "memory : 1\n",
      "clips : 1\n",
      "he : 1\n",
      "pines : 1\n",
      "ex-girlfriend : 1\n",
      "faith : 1\n",
      "juliette : 1\n",
      "lewis : 1\n",
      "but : 2\n",
      "does : 2\n",
      "n't : 1\n",
      "notice : 1\n",
      "that : 4\n",
      "another : 1\n",
      "friend : 1\n",
      "mace : 1\n",
      "angela : 1\n",
      "bassett : 1\n",
      "really : 1\n",
      "cares : 1\n",
      "him : 1\n",
      "this : 2\n",
      "film : 3\n",
      "features : 1\n",
      "good : 1\n",
      "performances : 1\n",
      "impressive : 1\n",
      "film-making : 2\n",
      "technique : 1\n",
      "and : 6\n",
      "breath-taking : 1\n",
      "crowd : 1\n",
      "scenes : 1\n",
      "director : 1\n",
      "kathryn : 1\n",
      "bigelow : 1\n",
      "knows : 1\n",
      "her : 1\n",
      "stuff : 1\n",
      "not : 2\n",
      "hesitate : 1\n",
      "to : 2\n",
      "use : 1\n",
      "it : 3\n",
      "a : 2\n",
      "whole : 1\n",
      "is : 2\n",
      "an : 1\n",
      "unsatisfying : 1\n",
      "movie : 1\n",
      "problem : 1\n",
      "writers : 1\n",
      "james : 1\n",
      "cameron : 1\n",
      "jay : 1\n",
      "cocks : 1\n",
      "were : 1\n",
      "too : 1\n",
      "ambitious : 2\n",
      "aiming : 1\n",
      "with : 1\n",
      "social : 1\n",
      "relevance : 1\n",
      "thrills : 1\n",
      "drama : 1\n",
      "should : 1\n",
      "be : 1\n",
      "discouraged : 1\n",
      "; : 1\n",
      "just : 2\n",
      "when : 1\n",
      "fails : 2\n",
      "achieve : 1\n",
      "its : 1\n",
      "goals : 1\n",
      "badly : 1\n",
      "obviously : 1\n",
      "ends : 1\n",
      "preachy : 1\n",
      "unexciting : 1\n",
      "uninvolving : 1\n"
     ]
    }
   ],
   "source": [
    "# 3.5.1 Get token frequency\n",
    "\n",
    "# get unigram frequency \n",
    "# recall, you can also get the dictionary by \n",
    "# {token:count(token) for token in set(tokens)}\n",
    "\n",
    "word_dist=nltk.FreqDist(tokens)\n",
    "print(\"word_dist:\", word_dist)\n",
    "\n",
    "# get the most frequent items\n",
    "print(\"top 10 words:\", word_dist.most_common(10))\n",
    "\n",
    "# what kind of words usually have high frequency?\n",
    "\n",
    "# it behaves as a dictionary\n",
    "for word in word_dist:\n",
    "    print(word,\":\", word_dist[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.5.1 Stop words and word filtering\n",
    "\n",
    " - Stop words: a set of commonly used words, have very little meaning, and cannot differentiate a text from others, such as \"and\", \"the\" etc. \n",
    " - Stop words are typically ignored in NLP processing or by search engine\n",
    " - Stop words usually are application specific. You can define your own stop words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sort dictionary without stop words by frequency\n",
      "[('days', 2), ('film-making', 2), ('ambitious', 2), ('fails', 2), ('strange', 1), ('chronicles', 1), ('last', 1), ('two', 1), ('1999', 1), ('los', 1), ('angeles', 1), ('locals', 1), ('gear', 1), ('new', 1), ('millenium', 1), ('lenny', 1), ('nero', 1), ('ralph', 1), ('fiennes', 1), ('goes', 1), ('business', 1), ('peddling', 1), ('erotic', 1), ('memory', 1), ('clips', 1), ('pines', 1), ('ex-girlfriend', 1), ('faith', 1), ('juliette', 1), ('lewis', 1), (\"n't\", 1), ('notice', 1), ('another', 1), ('friend', 1), ('mace', 1), ('angela', 1), ('bassett', 1), ('really', 1), ('cares', 1), ('features', 1), ('good', 1), ('performances', 1), ('impressive', 1), ('technique', 1), ('breath-taking', 1), ('crowd', 1), ('scenes', 1), ('director', 1), ('kathryn', 1), ('bigelow', 1), ('knows', 1), ('stuff', 1), ('hesitate', 1), ('use', 1), ('whole', 1), ('unsatisfying', 1), ('movie', 1), ('problem', 1), ('writers', 1), ('james', 1), ('cameron', 1), ('jay', 1), ('cocks', 1), ('aiming', 1), ('social', 1), ('relevance', 1), ('thrills', 1), ('drama', 1), ('discouraged', 1), ('achieve', 1), ('goals', 1), ('badly', 1), ('obviously', 1), ('ends', 1), ('preachy', 1), ('unexciting', 1), ('uninvolving', 1)]\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5.1.1\n",
    "# get NLTK English stop words\n",
    "# You can modify this list by adding more stop words or remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words+=[\"film\", \"films\"]\n",
    "#print (stop_words)\n",
    "\n",
    "# filter stop words out of the dictionary\n",
    "# by creating a new dictionary\n",
    "\n",
    "filtered_dict={word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words and\n",
    "                        word not in string.punctuation}\n",
    "\n",
    "print(\"\\nsort dictionary without stop words by frequency\")\n",
    "print(sorted(filtered_dict.items(), key=lambda item:-item[1]))\n",
    "\n",
    "print(len(filtered_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.2 positive/negative words: sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'positive-words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-eae99b117abe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Find positive words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"positive-words.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mpositive_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'positive-words.txt'"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5.2.1\n",
    "# Find positive words \n",
    "\n",
    "with open(\"positive-words.txt\",'r') as f:\n",
    "    positive_words=[line.strip() for line in f]\n",
    "\n",
    "#positive_words\n",
    "#print(positive_words)\n",
    "positive_tokens=[token for token in tokens \\\n",
    "                 if token in positive_words]\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Naive sentiment analysis**:\n",
    "  - Find positive/negative words\n",
    "  - If more positive words than negative, then positive\n",
    "  - Otherwise, negative\n",
    "- Note the sentence: \n",
    "  -  \"the problem is that the writers, james cameron and jay cocks , were **<font color=\"red\">too ambitious</font>**, aiming for a film with social relevance, thrills, and drama. **<font color=\"red\">not that ambitious</font>** film-making should be discouraged; just that when it fails to achieve its goals ...\"\n",
    "- How to deal with **negation**?\n",
    "- Some useful rules:\n",
    "    - Negative sentiment: \n",
    "      - negative words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - positive words preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "    - Positive sentiment (in the similar fashion):\n",
    "      - positive words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - negative terms following a negation within  $n$ (e.g. three) words in the same sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.5.2.2 # check if a positive word is preceded by negation words\n",
    "# e.g. not, too, n't, no, cannot\n",
    "\n",
    "# this is not an exhaustive list of negation words!\n",
    "negations=['not', 'too', 'n\\'t', 'no', 'cannot', 'neither','nor']\n",
    "tokens = nltk.word_tokenize(text)  \n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>0:\n",
    "            if tokens[idx-1] not in negations:\n",
    "                positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)\n",
    "\n",
    "# what if a positive word is preceded \n",
    "# by a negation within N words? \n",
    "# e.g. 'does not make any customer happy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
