{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Other NLP Packages: spaCy and Gensim</center>\n",
    "\n",
    "References: \n",
    "- https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "- https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. spaCy\n",
    "- spaCy is a relatively new framework in the Python Natural Language Processing, but is getting popular\n",
    "- Provides models for Part Of Speech tagging, Named Entity Recognition and Dependency Parsing\n",
    "- Supports 8 languages out of the box\n",
    "- Provides easy and beautiful visualizations\n",
    "- PProvides pretrained word vectors\n",
    "- installation:\n",
    "  1. pip install spacy\n",
    "  2. python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.1. Load package and language library\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.2. Get POS, lemmatization, and other NLP tasks all in one task\n",
    "\n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\".format(\n",
    "        token.text,         # original text\n",
    "        token.lemma_,       # lemma\n",
    "        token.is_punct,     # is it a punctuation ?\n",
    "        token.is_space,     # is it a space\n",
    "        token.pos_,         # The simple part-of-speech tag.\n",
    "        token.tag_          # The detailed part-of-speech tag\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.3. Segment by sentences\n",
    "\n",
    "doc = nlp(\"These are apples. These are oranges.\")\n",
    " \n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.4. Entity Recognition\n",
    "\n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.5. Visulaize named entities\n",
    "\n",
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "displacy.render(doc, style='ent', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.6. Visualized dependency graph\n",
    "\n",
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. gensim\n",
    "- Gensim is an open source Python library for NLP, with a focus on topic modeling.\n",
    "- It is not an everything-including-the-kitchen-sink NLP research library (like NLTK); instead, Gensim is a mature, focused, and efficient suite of NLP tools for topic modeling, including \n",
    "  - Word2Vec word embedding \n",
    "  - Topic modeling\n",
    "  - Text preprocessing like **phrase extraction**\n",
    "  \n",
    "- Gensim Phrase Model: \n",
    "    - **gensim.models.phrases.Phrases(sentences, min_count, threshold, max_vocab_size, delimiter, scoring, ...)**\n",
    "        - *sentences*: list of sentences or iterables, each of which can be a document\n",
    "        - *min_count*: Ignore all words and bigrams with total collected count lower than this value.\n",
    "        - *threshold*: Represent a score threshold for forming the phrases (higher means fewer phrases). A phrase of words $a$ followed by $b$ is accepted if the score of the phrase is greater than threshold. Heavily depends on concrete scoring-function.\n",
    "        - *max_vocab_size*: Maximum size (number of tokens) of the vocabulary. \n",
    "        - *delimiter*: Glue character used to join collocation tokens, should be a byte string (e.g. b’\\_’).\n",
    "        - *scoring: Specify how potential phrases are scored. \n",
    "           - **default** - original_scorer(), by Mikolov et al. (2013) (https://arxiv.org/pdf/1310.4546.pdf)\n",
    "           - **npmi** - npmi_scorer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.1. Find bigrams using gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# load a built-in NLTK corpus as a list of words\n",
    "words=nltk.corpus.gutenberg.words('austen-sense.txt')\n",
    "\n",
    "# Train phrase model to find phrases using original_scorer\n",
    "phrases = Phrases([words], min_count=5, threshold=10)\n",
    "\n",
    "for phrase, score in phrases.export_phrases([words]):\n",
    "    print(phrase, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.2. Find bigrams by NPMI\n",
    "\n",
    "# find phrases using NPMI\n",
    "\n",
    "phrases = Phrases([words], min_count=5, threshold=0.4, scoring='npmi')\n",
    "\n",
    "for phrase, score in phrases.export_phrases([words]):\n",
    "    print(phrase, score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.3. Tokenize by unigrams and bigrams\n",
    "\n",
    "# Initialize phrase tokenizer\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "sent=\"As dinner was not to be ready in less than two hours from their arrival,\"\n",
    "print(bigram[nltk.word_tokenize(sent.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
