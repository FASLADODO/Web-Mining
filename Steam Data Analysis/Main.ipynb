{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Steam Game Data Analytics</center>\n",
    "### <center>Web Mining Project</center>\n",
    "- **Mission statement:**\n",
    "    - Analyze the game review data on Steam platform and try to make some explorative deployment.\n",
    "        - Sentiment analytics\n",
    "        - Product development advice\n",
    "        - Player’s behavior analysis\n",
    "        - Game recommendation\n",
    "        - Popular game prediction\n",
    "- **Team Members:**\n",
    "    - Fanyu WANG.    fwang15@stevens.edu\n",
    "    - Fu YANG.       fyang24@stevens.edu\n",
    "    - Kai ZHANG.     kzhang32@stevens.edu\n",
    "    - Shuai WANG.    swang111@stevens.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrape Data from Steam Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dyamic Web Page Scraping with Selenium \n",
    "    - References:\n",
    "        - http://selenium-python.readthedocs.io/getting-started.html\n",
    "        - https://medium.com/the-andela-way/introduction-to-web-scraping-using-selenium-7ec377a8cf72\n",
    "- Scape HTML web pages using BeatifulSoup\n",
    "    - References: \n",
    "        - https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "        - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests package\n",
    "import requests                   \n",
    "\n",
    "# import BeautifulSoup from package bs4 (i.e. beautifulsoup4)\n",
    "from bs4 import BeautifulSoup    \n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅰ. Selenium\n",
    "- this is not a function, but you can run it independently to test the performance of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions\n",
    "# import time\n",
    "\n",
    "# executable_path = 'F:\\geckodriver'\n",
    "\n",
    "# driver = webdriver.Firefox(executable_path=executable_path)\n",
    "\n",
    "# driver.get('https://steamcommunity.com/app/435150/reviews/?p=1&browsefilter=toprated')\n",
    "\n",
    "# src_updated = driver.page_source\n",
    "# src = \"\"\n",
    "\n",
    "# for i in range(0,190):\n",
    "#     if src != src_updated:\n",
    "#         # save page source (i.e. html document) before page-down\n",
    "#         src = src_updated\n",
    "#         # execute javascript to scroll to the bottom of the window\n",
    "#         # you can also use page-down\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         # sleep to allow content loaded\n",
    "#         time.sleep(1)\n",
    "#         # save page source after page-down\n",
    "#         src_updated = driver.page_source\n",
    "\n",
    "\n",
    "# # What can we get about the reviews from Steam?\n",
    "\n",
    "# # The review content, post date, how many people think this review helpful or funny\n",
    "\n",
    "# review_content=[]\n",
    "# usefuls=[]\n",
    "\n",
    "# for i in range(1,191):\n",
    "#     # get all Q&A list using XPATH locator\n",
    "#     lists=driver.find_elements_by_xpath(\"//div[@id='page%d']/div\"%i)\n",
    "# #     print(\"//div[@id='page%d']/div\"%i)\n",
    "# #     print(\"page%d pairs: \"%i,len(lists))\n",
    "    \n",
    "#     for idx,item in enumerate(lists):    \n",
    "#     # each Q&A pair has an unique ID\n",
    "#         div_id=item.get_attribute(\"id\")\n",
    "#         content_css=\"div#\"+div_id+\" \"+\"div.apphub_UserReviewCardContent div.apphub_CardTextContent\"\n",
    "#         useful_css=\"div#\"+div_id+\" \"+\"div.apphub_UserReviewCardContent div.found_helpful\"\n",
    "        \n",
    "#         review=driver.find_element_by_css_selector(content_css)\n",
    "#         useful=driver.find_element_by_css_selector(useful_css)\n",
    "#         review_content.append(review.text)\n",
    "#         usefuls.append(useful.text)\n",
    "\n",
    "# print(\"Total reviews scraped: \", len(review_content), len(usefuls))\n",
    "# # print(review_content[0])\n",
    "# # print(usefuls[0])\n",
    "\n",
    "# import pandas as pd\n",
    "# import nltk\n",
    "# import re\n",
    "\n",
    "# reviews =[]\n",
    "# dates = []\n",
    "\n",
    "# # print(type(data[0]),str(data[0]))\n",
    "\n",
    "# for line in review_content:\n",
    "#     sentence = line.split(\"\\n\")\n",
    "#     date = re.sub(r\"Posted:\",\" \",sentence[0]).strip()\n",
    "#     review = re.sub(r\"Posted:.*\",\" \",line)\n",
    "#     review = re.sub(r\"\\s+\",\" \",review).strip() \n",
    "#     reviews.append(review)\n",
    "#     dates.append(date)\n",
    "    \n",
    "# # print(\"date:\",dates[0])\n",
    "# # print(\"content:\",reviews[0])\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "# df[\"review\"] = reviews\n",
    "# df[\"post_date\"] = dates\n",
    "# df[\"useful\"] = usefuls\n",
    "# df.to_csv('game_review.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅱ. BeatifulSoup\n",
    "**sometimes Selenium webdriver is not stable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape(address):\n",
    "    # store user_name \n",
    "    nicks=[]\n",
    "    # store recommendation information\n",
    "    titles=[]\n",
    "    # store hours the player played\n",
    "    hours=[]\n",
    "    # store review information\n",
    "    comments=[]\n",
    "    # store the number of people who thinks that this review is helpful\n",
    "    helpfuls=[]\n",
    "    # store the number of people who thinks that this review is funny\n",
    "    funnys=[]\n",
    "    # store the date the user commented\n",
    "    dates=[]\n",
    "    # store the number of games the user has\n",
    "    products=[]\n",
    "    # scrape 10 reviews per time\n",
    "    # therefore we scraped 2000 reviews\n",
    "    for i in range(1, 201):\n",
    "        # simulate the network requests\n",
    "        url =  address + '/homecontent/?userreviewsoffset=' + str(10 * (i - 1)) + '&p=' + str(i) + '&workshopitemspage=' + str(i) + '&readytouseitemspage=' + str(i) + '&mtxitemspage=' + str(i) + '&itemspage=' + str(i) + '&screenshotspage=' + str(i) + '&videospage=' + str(i) + '&artpage=' + str(i) + '&allguidepage=' + str(i) + '&webguidepage=' + str(i) + '&integratedguidepage=' + str(i) + '&discussionspage=' + str(i) + '&numperpage=10&browsefilter=toprated&browsefilter=toprated&appid=435150&appHubSubSection=10&l=senglish&filterLanguage=default&searchText=&forceanon=1'\n",
    "        html = requests.get(url).text.replace('<br>',' ')\n",
    "        soup = BeautifulSoup(html, 'html.parser') \n",
    "        # scrape the information which we focus on\n",
    "        reviews = soup.find_all('div', {'class': 'apphub_Card'})    \n",
    "        for review in reviews:\n",
    "            nick = review.find('div', {'class': 'apphub_CardContentAuthorName'})\n",
    "            nicks.append(nick.text)\n",
    "            title = review.find('div', {'class': 'title'}).text\n",
    "            titles.append(title)\n",
    "            hour = review.find('div', {'class': 'hours'}).text.split(' ')[0]\n",
    "            hours.append(hour)\n",
    "            product = review.find('div', {'class': 'apphub_CardContentMoreLink ellipsis'}).text.split()\n",
    "            # this content may be null. when it is null, we think it equals to zero\n",
    "            if len(product)==4:\n",
    "                products.append(product[0])\n",
    "            else:\n",
    "                products.append('0')\n",
    "            #link = nick.find('a').attrs['href']\n",
    "            comment = review.find('div', {'class': 'apphub_CardTextContent'}).text\n",
    "            temp=comment.split('\\n')\n",
    "            # there will be unwanted information. so we skip them.\n",
    "            if len(temp)==3:\n",
    "                comments.append(temp[2].strip('\\t'))\n",
    "            else:\n",
    "                comments.append(temp[3].strip('\\t'))\n",
    "            # delete string \"Posted: \" since it is unused\n",
    "            date = re.sub(r\"Posted:\",\" \",comment.split('\\n')[1].strip('\\t')).strip()\n",
    "            dates.append(date)\n",
    "            helpful = review.find('div', {'class': 'found_helpful'}).text.split()[0]\n",
    "            helpfuls.append(helpful)\n",
    "            #helpful1 = review.find('div', {'class': 'found_helpful'}).text.split()[6]\n",
    "            #funny = re.findall(r\"\\d+\",review.find('div', {'class': 'found_helpful'}).text.split()[5])\n",
    "            funny = review.find('div', {'class': 'found_helpful'}).text.split()\n",
    "            # this content may be null. when it is null, we think it equals to zero\n",
    "            if len(funny)==12:\n",
    "                funnys.append(funny[6])\n",
    "            else:\n",
    "                funnys.append('0')\n",
    "                \n",
    "    # generate dataframe to store web information\n",
    "    df = pd.DataFrame()\n",
    "    df[\"names\"] = nicks\n",
    "    df[\"products#\"] = products\n",
    "    df[\"marked as helpful\"] = helpfuls\n",
    "    df[\"marked as funny\"] = funnys\n",
    "    df[\"post_date\"] = dates\n",
    "    df[\"Recommend?\"] = titles\n",
    "    df[\"times on record\"] = hours\n",
    "    df[\"review\"] = comments\n",
    "    \n",
    "    # generate csv file\n",
    "    # df.to_csv('game_data_negative.csv',index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reviw Data Tokenization and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅰ. POS (Part of Speech) Tagging\n",
    "- The process of marking up a word in a text as corresponding to a particular part of speech (e.g. nouns, verbs, adjectives, adverbs etc.), based on both **its definition**, as well as its **context** — adjacent and related words in a phrase, sentence, or paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos_tag):\n",
    "    \n",
    "    # if pos tag starts with 'J'\n",
    "    if pos_tag.startswith('J'):\n",
    "        # return wordnet tag \"ADJ\"\n",
    "        return wordnet.ADJ\n",
    "\n",
    "    # if pos tag starts with 'V'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        # return wordnet tag \"VERB\"\n",
    "        return wordnet.VERB\n",
    "\n",
    "    # if pos tag starts with 'N'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        # return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # be default, return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅱ. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "    * Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \n",
    "    # Lemmatization: determining the lemma for a given word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Regular expression pattern\n",
    "    pattern = r'\\w[\\w\\'-]*\\w'\n",
    "    \n",
    "    init_reviews = []\n",
    "    init_helpfuls = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.append(\"game\")\n",
    "    \n",
    "    review = data[\"review\"].values.tolist()\n",
    "    helpfuls = data[\"marked as helpful\"].values.tolist()\n",
    "    \n",
    "    for doc in review:\n",
    "        doc = doc.lower()\n",
    "        tokens = nltk.regexp_tokenize(doc, pattern)\n",
    "        tagged_tokens= nltk.pos_tag(tokens)\n",
    "        lemmatized_words=[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \\\n",
    "              for (word, tag) in tagged_tokens \\\n",
    "              if word not in stop_words and word not in string.punctuation]\n",
    "        temp_str = \"\"\n",
    "        for item in lemmatized_words:\n",
    "            temp_str = temp_str + \" \" + item\n",
    "        init_reviews.append(temp_str[1:])\n",
    "    \n",
    "    # convert \"No\" to \"0\"\n",
    "    for line in helpfuls:\n",
    "        if line==\"No\":\n",
    "            init_helpfuls.append(\"0\")\n",
    "        else:\n",
    "            init_helpfuls.append(line)\n",
    "        \n",
    "    # Generate csv file to save data without unuseful information.\n",
    "    df = pd.DataFrame()\n",
    "    df[\"user_name\"] = data[\"names\"].values.tolist()\n",
    "    df[\"user_product\"] = data[\"products#\"].values.tolist()\n",
    "    df[\"helpful\"] = init_helpfuls\n",
    "    df[\"funny\"] = data[\"marked as funny\"].values.tolist()\n",
    "    df[\"post_date\"] = data[\"post_date\"].values.tolist()\n",
    "    df[\"recommend_or_not\"] = data[\"Recommend?\"].values.tolist()\n",
    "    df[\"game_time\"] = data[\"times on record\"].values.tolist()\n",
    "    df[\"review\"] = init_reviews\n",
    "    # obtain the row index when contents of reviews are empty\n",
    "    indx = df[df.review==''].index.tolist()\n",
    "    # delete the corresponding datasets\n",
    "    df1=df.drop(df.index[indx])\n",
    "    # delete the duplicate datasets\n",
    "    df1.drop_duplicates(subset =\"user_name\", \n",
    "                     keep = False, inplace = True)\n",
    "    df1.to_csv('C:/Users/yongk/Documents/PythonLearning/Steam Data Analysis/Data_main/tokened_normed_review_test.csv',index=False)\n",
    "    # this method generate a dataframe with tokenized data sets\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅲ. Merge mupltiple csv files into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv():\n",
    "    Folder_Path = r'C:\\Users\\yongk\\Documents\\PythonLearning\\Steam Data Analysis\\Data_main'          \n",
    "    SaveFile_Path =  r'C:\\Users\\yongk\\Documents\\PythonLearning\\Steam Data Analysis\\Data_main'       \n",
    "    SaveFile_Name = r'game_data_all.csv'              \n",
    " \n",
    "    os.chdir(Folder_Path)\n",
    "    # save file names into a list\n",
    "    file_list = os.listdir()\n",
    " \n",
    "    # read the first csv including headers\n",
    "    df = pd.read_csv(Folder_Path +'\\\\'+ file_list[0])   #default utf-8\n",
    " \n",
    "    # write the first csv to second one\n",
    "    df.to_csv(SaveFile_Path+'\\\\'+ SaveFile_Name,encoding=\"utf_8_sig\",index=False)\n",
    " \n",
    "    # loop through all files\n",
    "    for i in range(1,len(file_list)):\n",
    "        df = pd.read_csv(Folder_Path + '\\\\'+ file_list[i])\n",
    "        df.to_csv(SaveFile_Path+'\\\\'+ SaveFile_Name,encoding=\"utf_8_sig\",index=False, header=False, mode='a+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅳ. getdata function\n",
    "- generate a datafram to store all data\n",
    "- generate a csv file to store all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata():\n",
    "    # obtain data including positive and negative reviews\n",
    "    data_positive = web_scrape(address_positive)\n",
    "    data_negative = web_scrape(address_negative)\n",
    "    # concat two dataframes\n",
    "    data = pd.concat([data_positive,data_negative],ignore_index=True)\n",
    "    # generate csv files to store two dataframes\n",
    "    data_positive.to_csv('C:/Users/yongk/Documents/PythonLearning/Steam Data Analysis/Data_main/game_data_positive.csv',index=False)\n",
    "    data_negative.to_csv('C:/Users/yongk/Documents/PythonLearning/Steam Data Analysis/Data_main/game_data_negative.csv',index=False)\n",
    "    # merge multiple csv\n",
    "    merge_csv()# game_data_all.csv\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅰ. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**updating**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅱ. Term Frequency and Inverse Dcoument Frequency (TF-IDF) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to tokenalize normal review before we do TF-IDF #\n",
    "def get_doc_tokens(doc):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.append(\"game\")\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc) if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    # create token count dictionary\n",
    "    token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "\n",
    "# This function is to get TF-IDF matrix #\n",
    "def get_tf_idf(reviews):\n",
    "    \n",
    "    docs_tokens={idx:get_doc_tokens(doc) for idx,doc in enumerate(reviews)}\n",
    "\n",
    "    # since we have a small corpus, we can use dataframe to get document-term matrix\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "    # convert dtm to numpy arrays\n",
    "    tf=dtm.values\n",
    "\n",
    "    # sum the value of each row\n",
    "    doc_len=tf.sum(axis=1)\n",
    "\n",
    "    # divide dtm matrix by the doc length matrix\n",
    "    tf=np.divide(tf, doc_len[:,None])\n",
    "\n",
    "    # get document freqent\n",
    "    df=np.where(tf>0,1,0)\n",
    "\n",
    "    # get idf\n",
    "    smoothed_idf=np.log(np.divide(len(reviews)+1, np.sum(df, axis=0)+1))+1\n",
    "\n",
    "    # get tf-idf\n",
    "    smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "# This function is to get the most similarity review given a review_id #\n",
    "def find_similar_doc(doc_id, smoothed_tf_idf):\n",
    "    similarity=1-distance.squareform(distance.pdist(smoothed_tf_idf, 'cosine'))\n",
    "    \n",
    "    # find top doc similar to first one\n",
    "    best_matching_doc_id = np.argsort(similarity)[:,::-1][doc_id,0:2][1]\n",
    "    similarity = similarity[doc_id,best_matching_doc_id]  \n",
    "    return best_matching_doc_id, similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "- Collect datasets (positive and negative reviews)\n",
    "    * user_name, votes, how much products in account, hours to play and so on\n",
    "    * store data into a single dataframe\n",
    "    * store data into a signle csv file\n",
    "- Tokenize datasets\n",
    "    * for convinience, convert symbols to number. (eg. \"No\" to \"0\")\n",
    "    * remove the words in stop_words list (which is posted in class)\n",
    "    * remove the words (game and games) since the frequency is too high and they are negligible words\n",
    "    * remove the unrecognized words (they may be Chinese or other language, emoji and so on)\n",
    "    * remove the datasets with reviews which are empty after tokenization\n",
    "    * remove the duplicate datesets since there is an iterative scraping issue (we define there will be 10 reviews every page, but in fact, the number is random based on the length of reviews).\n",
    "- Data Analysis\n",
    "    - Sentiment Analysis\n",
    "    - TF-IDF analysis\n",
    "        - Generate TF-IDF Metrix\n",
    "        - Find the index of most similar document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3151\n",
      "1992\n",
      "Smoothed TF-IDF Matrix\n",
      "[[0.13617387 0.02614374 0.07240039 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01908747 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.06143779 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01204663 0.         ... 0.01937316 0.01937316 0.01937316]]\n",
      "(919, 0.21090363467253592)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "# Test \"Scrape Data from Steam Platform\"\n",
    "    address_positive = 'https://steamcommunity.com/app/435150/positivereviews'\n",
    "    # https://steamcommunity.com/app/435150/\n",
    "    address_negative = 'http://steamcommunity.com/app/435150/negativereviews'\n",
    "    data = getdata() # get raw datasets\n",
    "    print(len(data))\n",
    "    \n",
    "# Test \"Data Tokenization and normalization\"\n",
    "    tokenized_data = tokenize(data)\n",
    "    print(len(tokenized_data))\n",
    "    \n",
    "# Test \"Data Analysis\"\n",
    "    # Sentiment Analysis\n",
    "    \n",
    "    # TF-IDF Analysis\n",
    "    reviews = tokenized_data[\"review\"].values.tolist()\n",
    "    tf_idf = get_tf_idf(reviews)\n",
    "    print(\"Smoothed TF-IDF Matrix\")\n",
    "    print(tf_idf)\n",
    "    \n",
    "    print(find_similar_doc(1,tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
