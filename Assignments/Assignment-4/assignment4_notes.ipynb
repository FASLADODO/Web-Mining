{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COM-101   COMPUTERS\n",
      "COM-111   DATABASE\n",
      "COM-211   ALGORITHM\n",
      "MAT-103   STATISTICS learning\n",
      "MAT-102   STATISTICS\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"\"\"COM-101   COMPUTERS\n",
    "COM-111   DATABASE\n",
    "COM-211   ALGORITHM\n",
    "MAT-103   STATISTICS learning\n",
    "MAT-102   STATISTICS\"\"\"\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COM', 'COM', 'COM', 'COM']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['COM']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['STATISTICS']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['101']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['101', '103', '102']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['103']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.findall(\".\",text)\n",
    "re.findall(\"COM\",text)\n",
    "re.findall(\"^COM\",text)\n",
    "re.findall(\"STATISTICS$\",text)\n",
    "re.findall(\"101\",text)\n",
    "re.findall('10[123]',text)\n",
    "re.findall('10[^12]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COM-1', 'COM-111', 'COM-']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['COM-1', 'COM-111']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['COM-1', 'COM-1', 'COM-']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('COM-1*',text)\n",
    "re.findall('COM-1+',text)\n",
    "re.findall('COM-1?',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11', '11']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['111', '11']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['111']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('1{2}',text)\n",
    "re.findall('1{2,}',text)\n",
    "re.findall('1{3}',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101', '111', '211', '103', '102']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['learning']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['COM',\n",
       " 'COMPUTERS',\n",
       " 'COM',\n",
       " 'DATABASE',\n",
       " 'COM',\n",
       " 'ALGORITHM',\n",
       " 'MAT',\n",
       " 'STATISTICS',\n",
       " 'MAT',\n",
       " 'STATISTICS']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[0-9]+',text)\n",
    "re.findall('[a-z]+',text)\n",
    "re.findall('[A-Z]+',text)\n",
    "#re.findall('[a-zA-Z0-9]+',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COM-',\n",
       " '   COMPUTERS\\nCOM-',\n",
       " '   DATABASE\\nCOM-',\n",
       " '   ALGORITHM\\nMAT-',\n",
       " '   STATISTICS learning\\nMAT-',\n",
       " '   STATISTICS']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['101', '103', '102']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[^0-9]+',text)\n",
    "re.findall('101|102|103',text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COM',\n",
       " '101',\n",
       " 'COMPUTERS',\n",
       " 'COM',\n",
       " '111',\n",
       " 'DATABASE',\n",
       " 'COM',\n",
       " '211',\n",
       " 'ALGORITHM',\n",
       " 'MAT',\n",
       " '103',\n",
       " 'STATISTICS',\n",
       " 'learning',\n",
       " 'MAT',\n",
       " '102',\n",
       " 'STATISTICS']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '   ',\n",
       " '\\n',\n",
       " '-',\n",
       " '   ',\n",
       " '\\n',\n",
       " '-',\n",
       " '   ',\n",
       " '\\n',\n",
       " '-',\n",
       " '   ',\n",
       " ' ',\n",
       " '\\n',\n",
       " '-',\n",
       " '   ']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['M-',\n",
       " '1   ',\n",
       " 'S\\n',\n",
       " 'M-',\n",
       " '1   ',\n",
       " 'E\\n',\n",
       " 'M-',\n",
       " '1   ',\n",
       " 'M\\n',\n",
       " 'T-',\n",
       " '3   ',\n",
       " 'S ',\n",
       " 'g\\n',\n",
       " 'T-',\n",
       " '2   ']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+',text) #word character\n",
    "#re.findall('[a-zA-Z0-9]+',text)\n",
    "re.findall('\\W+',text)#non-word character\n",
    "re.findall('\\w\\W+',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find cat!\n",
      "The cat cat\n"
     ]
    }
   ],
   "source": [
    "text=\"The cat catches a rat\"\n",
    "\n",
    "match= re.search(r'.*cat',text)\n",
    "if match:\n",
    "    print (\"find cat!\")\n",
    "    print (match.group())\n",
    "else:\n",
    "    print (\"not found!\")\n",
    "# greedy match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'cat']\n",
      "['The', 'cat', 'catches', 'a', 'rat']\n"
     ]
    }
   ],
   "source": [
    "# findall function\n",
    "match=re.findall(r'cat',text)\n",
    "print(match)\n",
    "# split function\n",
    "match=re.split(r'\\W+',text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CAT CATches a rat\n"
     ]
    }
   ],
   "source": [
    "# sub function\n",
    "match=re.sub(r'cat', 'CAT', text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T[A-Z] [A-Z] [A-Z] [A-Z] [A-Z]\n"
     ]
    }
   ],
   "source": [
    "match=re.sub(r'[a-z]+',r'[A-Z]',text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question \n",
    "#how to capture the first cat but ignore the cat inside catch\n",
    "#match=re.find(r'cat',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'catches', 'a', 'rat']\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "words=re.split(r'\\W+',text)\n",
    "print(words)\n",
    "for word in words:\n",
    "    if word=='cat':\n",
    "        print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'catches', 'a', 'rat']\n"
     ]
    }
   ],
   "source": [
    "words=re.findall(r'\\w+',text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issac Newton\n",
      "Issac Newton\n",
      "Issac Newton, Scientist\n",
      "[('Issac', 'Newton', 'Scientist')]\n",
      "[('Issac', 'Newton', 'Scientist')]\n",
      "[('Issac', 'Newton', 'Scientist')]\n",
      "['Issac Newton, Scientist']\n"
     ]
    }
   ],
   "source": [
    "m=re.match(r'\\w+ \\w+','Issac Newton, Scientist')\n",
    "print(m.group())\n",
    "print(m.group(0)) # whole text\n",
    "#print(m.group(1))# no such group. r'(\\w+) (\\w+)'\n",
    "#print(m.group(2))\n",
    "m=re.match(r'(\\w+) (\\w+)[\\,] (\\w+)', 'Issac Newton, Scientist')\n",
    "print(m.group())\n",
    "\n",
    "m=re.findall(r'(\\w{5})[\\W]+(\\w{6})[\\W]+(\\w{9})','Issac Newton, Scientist')\n",
    "#m=re.findall(r'(\\w{5})[\\W+](\\w{6})[\\W+](\\w{9})','Issac Newton, Scientist')\n",
    "#m=re.findall(r'(\\w+)[\\s+](\\w+)[,](\\w+)','Issac Newton, Scientist')\n",
    "print(m)\n",
    "m=re.findall(r'(\\w{5}) (\\w{6}), (\\w{9})','Issac Newton, Scientist')\n",
    "print(m)\n",
    "m=re.findall(r'(\\w{5})\\W+(\\w{6})\\W+(\\w{9})','Issac Newton, Scientist')\n",
    "print(m)\n",
    "m=re.findall(r'\\w{5}\\W+\\w{6}\\W+\\w{9}','Issac Newton, Scientist')\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201-959-5599\n",
      "['201-959-5599', '201-966-5599']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.2. find phone number\n",
    "text = \"201-959-5599 # This is Phone Number 201-966-5599\"\n",
    "\n",
    "phone=re.match(r'\\d{3}-\\d{3}-\\d{4}',text)\n",
    "print(phone.group())\n",
    "\n",
    "phone=re.findall(r'\\d{3}-\\d{3}-\\d{4}',text)\n",
    "print(phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['201.959-5599']\n",
      "['201.959-5599', '201-966-5599']\n",
      "[('201', '959', '5599'), ('201', '966', '5599')]\n"
     ]
    }
   ],
   "source": [
    "# How about phone numbers like 201.959.5599?\n",
    "text = \"201.959-5599 # This is Phone Number 201-966-5599\"\n",
    "\n",
    "a=re.findall(r'\\d{3}\\.\\d{3}-\\d{4}',text)\n",
    "print(a)\n",
    "phones2 = re.findall(r'\\d{3}[-\\.]\\d{3}[-\\.]\\d{4}', text)\n",
    "print(phones2)\n",
    "b=re.findall(r'(\\d{3})[-\\.](\\d{3})[-\\.](\\d{4})',text)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joe.doe@example1.com', 'abc-xyz@example2.edu']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.3. find email address\n",
    "text = \"email me at joe.doe@example1.com or at abc-xyz@example2.edu\"\n",
    "\n",
    "a=re.findall(r'[a-zA-Z0-9._-]+@[a-zA-Z0-9.-_]+',text)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('COM-101', 'COMPUTERS'), ('COM-111', 'DATABASE'), ('COM-211', 'ALGORITHM'), ('MAT-103', 'STATISTICS'), ('MAT-102', 'STATISTICS')]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.4. Extract course name and title as \n",
    "# [('COM-101', 'COMPUTERS'),\n",
    "#  ('COM-111', 'DATABASE'),\n",
    "#  ... ]\n",
    "\n",
    "text = '''COM-101   COMPUTERS\n",
    "COM-111   DATABASE\n",
    "COM-211   ALGORITHM\n",
    "MAT-103   STATISTICS learning\n",
    "MAT-102   STATISTICS'''\n",
    "\n",
    "a=re.findall(r'(\\w+-\\d+)\\s+(\\w+)',text)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''`strange days' chronicles the last two days of 1999 in los angeles. \n",
    " as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips. \n",
    " he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him. \n",
    " this film features good performances, impressive film-making technique and breath-taking crowd scenes. \n",
    " director kathryn bigelow knows her stuff and does not hesitate to use it. \n",
    " but as a whole, this is an unsatisfying movie. \n",
    " the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \n",
    " not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \n",
    " the film just ends up preachy, unexciting and uninvolving.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`strange days' chronicles the last two days of 1999 in los angeles. \\n as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips. \\n he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him. \\n this film features good performances, impressive film-making technique and breath-taking crowd scenes. \\n director kathryn bigelow knows her stuff and does not hesitate to use it. \\n but as a whole, this is an unsatisfying movie. \\n the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \\n not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \\n the film just ends up preachy, unexciting and uninvolving.\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'strange', 'days', 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', 'lenny', 'nero', 'ralph', 'fiennes', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', 'he', 'pines', 'for', 'his', 'ex', 'girlfriend', 'faith', 'juliette', 'lewis', 'but', 'doesn', 't', 'notice', 'that', 'another', 'friend', 'mace', 'angela', 'bassett', 'really', 'cares', 'for', 'him', 'this', 'film', 'features', 'good', 'performances', 'impressive', 'film', 'making', 'technique', 'and', 'breath', 'taking', 'crowd', 'scenes', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', 'but', 'as', 'a', 'whole', 'this', 'is', 'an', 'unsatisfying', 'movie', 'the', 'problem', 'is', 'that', 'the', 'writers', 'james', 'cameron', 'and', 'jay', 'cocks', 'were', 'too', 'ambitious', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', 'thrills', 'and', 'drama', 'not', 'that', 'ambitious', 'film', 'making', 'should', 'be', 'discouraged', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', 'it', 'fails', 'badly', 'and', 'obviously', 'the', 'film', 'just', 'ends', 'up', 'preachy', 'unexciting', 'and', 'uninvolving', '']\n"
     ]
    }
   ],
   "source": [
    "tokens=re.split('\\W+',text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['`', 'strange', 'days', \"'\", 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', '.', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', ',', 'lenny', 'nero', '(', 'ralph', 'fiennes', ')', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', '.', 'he', 'pines', 'for', 'his', 'ex-girlfriend', ',', 'faith', '(', 'juliette', 'lewis', ')', ',', 'but', 'does', \"n't\", 'notice', 'that', 'another', 'friend', ',', 'mace', '(', 'angela', 'bassett', ')', 'really', 'cares', 'for', 'him', '.', 'this', 'film', 'features', 'good', 'performances', ',', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', '.', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', '.', 'but', 'as', 'a', 'whole', ',', 'this', 'is', 'an', 'unsatisfying', 'movie', '.', 'the', 'problem', 'is', 'that', 'the', 'writers', ',', 'james', 'cameron', 'and', 'jay', 'cocks', ',', 'were', 'too', 'ambitious', ',', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', ',', 'thrills', ',', 'and', 'drama', '.', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', ';', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', ',', 'it', 'fails', 'badly', 'and', 'obviously', '.', 'the', 'film', 'just', 'ends', 'up', 'preachy', ',', 'unexciting', 'and', 'uninvolving', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[token.strip(string.punctuation) for token in tokens]\n",
    "#print(tokens)\n",
    "tokens=[token.strip() for token in tokens if token.strip()!='']\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"strange days' chronicles the last two days of 1999 in los angeles.\", 'as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips.', \"he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him.\", 'this film features good performances, impressive film-making technique and breath-taking crowd scenes.', 'director kathryn bigelow knows her stuff and does not hesitate to use it.', 'but as a whole, this is an unsatisfying movie.', 'the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama.', 'not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously.', 'the film just ends up preachy, unexciting and uninvolving.']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.5 Use NLTK's regular expression tokenizer \n",
    "# to define sentences, i.e. \n",
    "# (1) starts with non-space character, \n",
    "# (2) contains any number of characters in the middle, \n",
    "#     as long as they are not \"!?.\"\n",
    "# (3) ends with !?.\n",
    "pattern= r'\\w[^!?.]*[!?.]'\n",
    "tokens1=nltk.regexp_tokenize(text, pattern)\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('`', 'strange'), ('strange', 'days'), ('days', \"'\"), (\"'\", 'chronicles'), ('chronicles', 'the'), ('the', 'last'), ('last', 'two'), ('two', 'days'), ('days', 'of'), ('of', '1999'), ('1999', 'in'), ('in', 'los'), ('los', 'angeles'), ('angeles', '.'), ('.', 'as'), ('as', 'the'), ('the', 'locals'), ('locals', 'gear'), ('gear', 'up'), ('up', 'for'), ('for', 'the'), ('the', 'new'), ('new', 'millenium'), ('millenium', ','), (',', 'lenny'), ('lenny', 'nero'), ('nero', '('), ('(', 'ralph'), ('ralph', 'fiennes'), ('fiennes', ')'), (')', 'goes'), ('goes', 'about'), ('about', 'his'), ('his', 'business'), ('business', 'of'), ('of', 'peddling'), ('peddling', 'erotic'), ('erotic', 'memory'), ('memory', 'clips'), ('clips', '.'), ('.', 'he'), ('he', 'pines'), ('pines', 'for'), ('for', 'his'), ('his', 'ex-girlfriend'), ('ex-girlfriend', ','), (',', 'faith'), ('faith', '('), ('(', 'juliette'), ('juliette', 'lewis'), ('lewis', ')'), (')', ','), (',', 'but'), ('but', 'does'), ('does', \"n't\"), (\"n't\", 'notice'), ('notice', 'that'), ('that', 'another'), ('another', 'friend'), ('friend', ','), (',', 'mace'), ('mace', '('), ('(', 'angela'), ('angela', 'bassett'), ('bassett', ')'), (')', 'really'), ('really', 'cares'), ('cares', 'for'), ('for', 'him'), ('him', '.'), ('.', 'this'), ('this', 'film'), ('film', 'features'), ('features', 'good'), ('good', 'performances'), ('performances', ','), (',', 'impressive'), ('impressive', 'film-making'), ('film-making', 'technique'), ('technique', 'and'), ('and', 'breath-taking'), ('breath-taking', 'crowd'), ('crowd', 'scenes'), ('scenes', '.'), ('.', 'director'), ('director', 'kathryn'), ('kathryn', 'bigelow'), ('bigelow', 'knows'), ('knows', 'her'), ('her', 'stuff'), ('stuff', 'and'), ('and', 'does'), ('does', 'not'), ('not', 'hesitate'), ('hesitate', 'to'), ('to', 'use'), ('use', 'it'), ('it', '.'), ('.', 'but'), ('but', 'as'), ('as', 'a'), ('a', 'whole'), ('whole', ','), (',', 'this'), ('this', 'is'), ('is', 'an'), ('an', 'unsatisfying'), ('unsatisfying', 'movie'), ('movie', '.'), ('.', 'the'), ('the', 'problem'), ('problem', 'is'), ('is', 'that'), ('that', 'the'), ('the', 'writers'), ('writers', ','), (',', 'james'), ('james', 'cameron'), ('cameron', 'and'), ('and', 'jay'), ('jay', 'cocks'), ('cocks', ','), (',', 'were'), ('were', 'too'), ('too', 'ambitious'), ('ambitious', ','), (',', 'aiming'), ('aiming', 'for'), ('for', 'a'), ('a', 'film'), ('film', 'with'), ('with', 'social'), ('social', 'relevance'), ('relevance', ','), (',', 'thrills'), ('thrills', ','), (',', 'and'), ('and', 'drama'), ('drama', '.'), ('.', 'not'), ('not', 'that'), ('that', 'ambitious'), ('ambitious', 'film-making'), ('film-making', 'should'), ('should', 'be'), ('be', 'discouraged'), ('discouraged', ';'), (';', 'just'), ('just', 'that'), ('that', 'when'), ('when', 'it'), ('it', 'fails'), ('fails', 'to'), ('to', 'achieve'), ('achieve', 'its'), ('its', 'goals'), ('goals', ','), (',', 'it'), ('it', 'fails'), ('fails', 'badly'), ('badly', 'and'), ('and', 'obviously'), ('obviously', '.'), ('.', 'the'), ('the', 'film'), ('film', 'just'), ('just', 'ends'), ('ends', 'up'), ('up', 'preachy'), ('preachy', ','), (',', 'unexciting'), ('unexciting', 'and'), ('and', 'uninvolving'), ('uninvolving', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.3.1. Get bigrams from the text                       \n",
    "\n",
    "# bigrams are formed from unigrams\n",
    "# nltk.bigram returns an iterator\n",
    "\n",
    "tokens=nltk.word_tokenize(text)\n",
    "#print(tokens)\n",
    "bigrams=list(nltk.bigrams(tokens))\n",
    "print(bigrams)\n",
    "trigrams=list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.metrics.association.BigramAssocMeasures object at 0x000001F801AF0EB8>\n",
      "<nltk.collocations.BigramCollocationFinder object at 0x000001F801AF0F98>\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "# bigram association measures\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "print(bigram_measures)\n",
    "# construct bigrams using words from our example\n",
    "finder = BigramCollocationFinder.from_words(tokens) # tokens are created in Exercise 3.1.4\n",
    "print(finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 'the'),\n",
       " ('it', 'fails'),\n",
       " (\"'\", 'chronicles'),\n",
       " ('(', 'angela'),\n",
       " ('(', 'juliette'),\n",
       " ('(', 'ralph'),\n",
       " (')', ','),\n",
       " (')', 'goes'),\n",
       " (')', 'really'),\n",
       " (',', 'aiming')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the corpus is too small\n",
    "finder.nbest(bigram_measures.raw_freq, 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'and'),\n",
       " (',', '\"'),\n",
       " ('of', 'the'),\n",
       " (\"'\", 's'),\n",
       " ('in', 'the'),\n",
       " ('said', ','),\n",
       " ('said', 'to'),\n",
       " ('.', 'He'),\n",
       " ('the', 'land'),\n",
       " ('.', 'The')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct bigrams using words from a large bulit-in NLTK corpus\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(\\\n",
    "        nltk.corpus.genesis.words('english-web.txt'))\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.collocations.BigramCollocationFinder object at 0x000001F80186CDD8>\n"
     ]
    }
   ],
   "source": [
    "print(finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('God', 'said'),\n",
       " ('one', 'hundred'),\n",
       " ('Jacob', 'said'),\n",
       " ('Yahweh', 'God'),\n",
       " ('Yahweh', 'said'),\n",
       " ('years', 'old'),\n",
       " ('seven', 'years'),\n",
       " ('Joseph', 'said'),\n",
       " ('every', 'man'),\n",
       " ('five', 'years')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.2. Find collocation by filter\n",
    "\n",
    "import string\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words\\\n",
    "                         or w.strip(string.punctuation)=='')\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('burnt', 'offering'),\n",
       " ('Paddan', 'Aram'),\n",
       " ('living', 'creature'),\n",
       " ('young', 'lady'),\n",
       " ('little', 'ones'),\n",
       " ('still', 'alive'),\n",
       " ('savory', 'food'),\n",
       " ('creeping', 'thing'),\n",
       " ('find', 'favor'),\n",
       " ('chief', 'cupbearer')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4.1.2 filter bigrams by frequency\n",
    "\n",
    "finder.apply_freq_filter(5)\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dist: <FreqDist with 115 samples and 175 outcomes>\n",
      "top 10 words: [(',', 13), ('.', 9), ('the', 6), ('and', 6), ('for', 4), ('that', 4), ('(', 3), (')', 3), ('film', 3), ('it', 3)]\n",
      "` : 1\n",
      "strange : 1\n",
      "days : 2\n",
      "' : 1\n",
      "chronicles : 1\n",
      "the : 6\n",
      "last : 1\n",
      "two : 1\n",
      "of : 2\n",
      "1999 : 1\n",
      "in : 1\n",
      "los : 1\n",
      "angeles : 1\n",
      ". : 9\n",
      "as : 2\n",
      "locals : 1\n",
      "gear : 1\n",
      "up : 2\n",
      "for : 4\n",
      "new : 1\n",
      "millenium : 1\n",
      ", : 13\n",
      "lenny : 1\n",
      "nero : 1\n",
      "( : 3\n",
      "ralph : 1\n",
      "fiennes : 1\n",
      ") : 3\n",
      "goes : 1\n",
      "about : 1\n",
      "his : 2\n",
      "business : 1\n",
      "peddling : 1\n",
      "erotic : 1\n",
      "memory : 1\n",
      "clips : 1\n",
      "he : 1\n",
      "pines : 1\n",
      "ex-girlfriend : 1\n",
      "faith : 1\n",
      "juliette : 1\n",
      "lewis : 1\n",
      "but : 2\n",
      "does : 2\n",
      "n't : 1\n",
      "notice : 1\n",
      "that : 4\n",
      "another : 1\n",
      "friend : 1\n",
      "mace : 1\n",
      "angela : 1\n",
      "bassett : 1\n",
      "really : 1\n",
      "cares : 1\n",
      "him : 1\n",
      "this : 2\n",
      "film : 3\n",
      "features : 1\n",
      "good : 1\n",
      "performances : 1\n",
      "impressive : 1\n",
      "film-making : 2\n",
      "technique : 1\n",
      "and : 6\n",
      "breath-taking : 1\n",
      "crowd : 1\n",
      "scenes : 1\n",
      "director : 1\n",
      "kathryn : 1\n",
      "bigelow : 1\n",
      "knows : 1\n",
      "her : 1\n",
      "stuff : 1\n",
      "not : 2\n",
      "hesitate : 1\n",
      "to : 2\n",
      "use : 1\n",
      "it : 3\n",
      "a : 2\n",
      "whole : 1\n",
      "is : 2\n",
      "an : 1\n",
      "unsatisfying : 1\n",
      "movie : 1\n",
      "problem : 1\n",
      "writers : 1\n",
      "james : 1\n",
      "cameron : 1\n",
      "jay : 1\n",
      "cocks : 1\n",
      "were : 1\n",
      "too : 1\n",
      "ambitious : 2\n",
      "aiming : 1\n",
      "with : 1\n",
      "social : 1\n",
      "relevance : 1\n",
      "thrills : 1\n",
      "drama : 1\n",
      "should : 1\n",
      "be : 1\n",
      "discouraged : 1\n",
      "; : 1\n",
      "just : 2\n",
      "when : 1\n",
      "fails : 2\n",
      "achieve : 1\n",
      "its : 1\n",
      "goals : 1\n",
      "badly : 1\n",
      "obviously : 1\n",
      "ends : 1\n",
      "preachy : 1\n",
      "unexciting : 1\n",
      "uninvolving : 1\n"
     ]
    }
   ],
   "source": [
    "# 3.5.1 Get token frequency\n",
    "\n",
    "# get unigram frequency \n",
    "# recall, you can also get the dictionary by \n",
    "# {token:count(token) for token in set(tokens)}\n",
    "\n",
    "word_dist=nltk.FreqDist(tokens)\n",
    "print(\"word_dist:\", word_dist)\n",
    "\n",
    "# get the most frequent items\n",
    "print(\"top 10 words:\", word_dist.most_common(10))\n",
    "\n",
    "# what kind of words usually have high frequency?\n",
    "\n",
    "# it behaves as a dictionary\n",
    "for word in word_dist:\n",
    "    print(word,\":\", word_dist[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'film', 'films']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words+=[\"film\", \"films\"]\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5\n"
     ]
    }
   ],
   "source": [
    "y=lambda x:-x\n",
    "print(y(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sort dictionary without stop words by frequency\n",
      "[('days', 2), ('film-making', 2), ('ambitious', 2), ('fails', 2), ('strange', 1), ('chronicles', 1), ('last', 1), ('two', 1), ('1999', 1), ('los', 1), ('angeles', 1), ('locals', 1), ('gear', 1), ('new', 1), ('millenium', 1), ('lenny', 1), ('nero', 1), ('ralph', 1), ('fiennes', 1), ('goes', 1), ('business', 1), ('peddling', 1), ('erotic', 1), ('memory', 1), ('clips', 1), ('pines', 1), ('ex-girlfriend', 1), ('faith', 1), ('juliette', 1), ('lewis', 1), (\"n't\", 1), ('notice', 1), ('another', 1), ('friend', 1), ('mace', 1), ('angela', 1), ('bassett', 1), ('really', 1), ('cares', 1), ('features', 1), ('good', 1), ('performances', 1), ('impressive', 1), ('technique', 1), ('breath-taking', 1), ('crowd', 1), ('scenes', 1), ('director', 1), ('kathryn', 1), ('bigelow', 1), ('knows', 1), ('stuff', 1), ('hesitate', 1), ('use', 1), ('whole', 1), ('unsatisfying', 1), ('movie', 1), ('problem', 1), ('writers', 1), ('james', 1), ('cameron', 1), ('jay', 1), ('cocks', 1), ('aiming', 1), ('social', 1), ('relevance', 1), ('thrills', 1), ('drama', 1), ('discouraged', 1), ('achieve', 1), ('goals', 1), ('badly', 1), ('obviously', 1), ('ends', 1), ('preachy', 1), ('unexciting', 1), ('uninvolving', 1)]\n"
     ]
    }
   ],
   "source": [
    "filtered_dict={word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words and\n",
    "                        word not in string.punctuation}\n",
    "\n",
    "print(\"\\nsort dictionary without stop words by frequency\")\n",
    "#print(filtered_dict.items())\n",
    "#print('\\n')\n",
    "print(sorted(filtered_dict.items(), key=lambda item:-item[1]))\n",
    "#print(sorted(filtered_dict.items()))\n",
    "#print(len(filtered_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['faith', 'good', 'impressive', 'ambitious', 'thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "with open(\"positive-words.txt\",'r') as f:\n",
    "    positive_words=[line.strip() for line in f]\n",
    "\n",
    "#positive_words\n",
    "#print(positive_words)\n",
    "positive_tokens=[token for token in tokens \\\n",
    "                 if token in positive_words]\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['`', 'strange', 'days', \"'\", 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', '.', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', ',', 'lenny', 'nero', '(', 'ralph', 'fiennes', ')', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', '.', 'he', 'pines', 'for', 'his', 'ex-girlfriend', ',', 'faith', '(', 'juliette', 'lewis', ')', ',', 'but', 'does', \"n't\", 'notice', 'that', 'another', 'friend', ',', 'mace', '(', 'angela', 'bassett', ')', 'really', 'cares', 'for', 'him', '.', 'this', 'film', 'features', 'good', 'performances', ',', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', '.', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', '.', 'but', 'as', 'a', 'whole', ',', 'this', 'is', 'an', 'unsatisfying', 'movie', '.', 'the', 'problem', 'is', 'that', 'the', 'writers', ',', 'james', 'cameron', 'and', 'jay', 'cocks', ',', 'were', 'too', 'ambitious', ',', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', ',', 'thrills', ',', 'and', 'drama', '.', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', ';', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', ',', 'it', 'fails', 'badly', 'and', 'obviously', '.', 'the', 'film', 'just', 'ends', 'up', 'preachy', ',', 'unexciting', 'and', 'uninvolving', '.']\n",
      "['faith', 'good', 'impressive', 'thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "# this is not an exhaustive list of negation words!\n",
    "negations=['not', 'too', 'n\\'t', 'no', 'cannot', 'neither','nor']\n",
    "tokens = nltk.word_tokenize(text)  \n",
    "\n",
    "print(tokens)\n",
    "\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>0:\n",
    "            if tokens[idx-1] not in negations:\n",
    "                positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['faith', 'good', 'impressive', 'ambitious', 'thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "N=3;\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>N-1:\n",
    "            for n in range(1,N+1):\n",
    "                if (tokens[idx-n] not in negations):\n",
    "                    if n==N:\n",
    "                        positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)\n",
    "# why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Q1\n"
     ]
    }
   ],
   "source": [
    "text='''contact Yahoo! at \"http://login.yahoo.com\", select forgot\n",
    "your password. If that fails to reset, contact Yahoo! at\n",
    "their password department 408-349-1572 -- Can't promise\n",
    "their phone department will fix, but they'll know where to\n",
    "go next. Corporate emails from Yahoo! don't come from\n",
    "their free mail system address space. Webmaster@yahoo.com\n",
    "is not a corporate email address.'''\n",
    "print(\"Test Q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contact yahoo! at \"http://login.yahoo.com\", select forgot\\nyour password. if that fails to reset, contact yahoo! at\\ntheir password department 408-349-1572 -- can\\'t promise\\ntheir phone department will fix, but they\\'ll know where to\\ngo next. corporate emails from yahoo! don\\'t come from\\ntheir free mail system address space. webmaster@yahoo.com\\nis not a corporate email address.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contact', 'yahoo', 'at', 'http', 'login.yahoo.com', 'select', 'forgot', 'your', 'password', 'if', 'that', 'fails', 'to', 'reset', 'contact', 'yahoo', 'at', 'their', 'password', 'department', '408-349-1572', 'can', 'promise', 'their', 'phone', 'department', 'will', 'fix', 'but', 'they', 'll', 'know', 'where', 'to', 'go', 'next', 'corporate', 'emails', 'from', 'yahoo', 'don', 'come', 'from', 'their', 'free', 'mail', 'system', 'address', 'space', 'webmaster@yahoo.com', 'is', 'not', 'corporate', 'email', 'address']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wordnet_lemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-831cc1165ac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m lemmatized_words=[wordnet_lemmatizer.lemmatize          (word, get_wordnet_pos(tag))           # tagged_tokens is a list of tuples (word, tag)\n\u001b[1;32m---> 45\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m           \u001b[1;31m# remove stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-831cc1165ac0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m           \u001b[1;31m# remove punctuations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m           word not in string.punctuation]\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mword_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordnet_lemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "#convert the string to lowercase\n",
    "text.lower()\n",
    "#Each token has at least two characters.\n",
    "#The first/last character can only be a letter (i.e. a-z) or a number (0-9)\n",
    "#In the middle, there are 0 or more characters, which can only be letters (a-z),\n",
    "#numbers (0-9), hyphens (\"-\"), underscores (\"_\"), dot (\".\"), or \"@\" symbols.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "pattern=r'\\w+[\\w.\\-_@]*\\w+'   \n",
    "tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "#print(len(tokens))\n",
    "print (tokens)\n",
    "lemmtokens=[]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# then find the POS tag of each word\n",
    "# tagged_token is a list of (word, pos_tag)\n",
    "tagged_tokens= nltk.pos_tag(tokens)\n",
    "stop_words = stopwords.words('english')\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \n",
    "    # if pos tag starts with 'J'\n",
    "    if pos_tag.startswith('J'):\n",
    "        # return wordnet tag \"ADJ\"\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    # if pos tag starts with 'V'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        # return wordnet tag \"VERB\"\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    # if pos tag starts with 'N'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        # return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # be default, return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "\n",
    "lemmatized_words=[wordnet_lemmatizer.lemmatize\\\n",
    "          (word, get_wordnet_pos(tag)) \\\n",
    "          # tagged_tokens is a list of tuples (word, tag)\n",
    "          for (word, tag) in tagged_tokens \\\n",
    "          # remove stop words\n",
    "          if word not in stop_words and \\\n",
    "          # remove punctuations\n",
    "          word not in string.punctuation]\n",
    "print(lemmatized_words)\n",
    "word_dist=nltk.FreqDist(lemmatized_words)\n",
    "word_dist\n",
    "#for token in tokens:\n",
    "    #lemmtoken=lemmatizer.lemmatize(token,wordnet.VERB)\n",
    "    #print(lemmtoken)\n",
    "    #lemmtokens.append(lemmtoken)\n",
    "#print(lemmtokens)\n",
    "#import string\n",
    "\n",
    "#stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 13, '.': 9, 'the': 6, 'and': 6, 'for': 4, 'that': 4, '(': 3, ')': 3, 'film': 3, 'it': 3, ...})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fails (verb) fail\n",
      "fails (verb) fails\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('fails (verb)', \\\n",
    "      wordnet_lemmatizer.lemmatize('fails', \\\n",
    "                                   wordnet.VERB))\n",
    "print('fails (verb)', \\\n",
    "      wordnet_lemmatizer.lemmatize('fails'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    token_count = None\n",
    "    # add your code here\n",
    "    \n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Q1\n"
     ]
    }
   ],
   "source": [
    "# Test Q1\n",
    "text='''contact Yahoo! at \"http://login.yahoo.com\", select forgot\n",
    "your password. If that fails to reset, contact Yahoo! at\n",
    "their password department 408-349-1572 -- Can't promise\n",
    "their phone department will fix, but they'll know where to\n",
    "go next. Corporate emails from Yahoo! don't come from\n",
    "their free mail system address space. Webmaster@yahoo.com\n",
    "is not a corporate email address.'''\n",
    "print(\"Test Q1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor : Kai Zhang\\nStevens Institute of Technology\\nAssignment 4: Natural Language Processing\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Q1\n",
      "contact 2\n",
      "yahoo 3\n",
      "http 1\n",
      "login.yahoo.com 1\n",
      "select 1\n",
      "forget 1\n",
      "password 2\n",
      "fail 1\n",
      "reset 1\n",
      "department 2\n",
      "408-349-1572 1\n",
      "promise 1\n",
      "phone 1\n",
      "fix 1\n",
      "know 1\n",
      "go 1\n",
      "next 1\n",
      "corporate 2\n",
      "email 2\n",
      "come 1\n",
      "free 1\n",
      "mail 1\n",
      "system 1\n",
      "address 2\n",
      "space 1\n",
      "webmaster@yahoo.com 1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Author : Kai Zhang\n",
    "Stevens Institute of Technology\n",
    "Assignment 4: Natural Language Processing\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Q1\n",
    "def tokenize(text):\n",
    "    #token_count = None\n",
    "    # add your code here\n",
    "    #convert the string to lowercase\n",
    "    #text.lower()\n",
    "    #Each token has at least two characters.\n",
    "    #The first/last character can only be a letter (i.e. a-z) or a number (0-9)\n",
    "    #In the middle, there are 0 or more characters, which can only be letters (a-z),\n",
    "    #numbers (0-9), hyphens (\"-\"), underscores (\"_\"), dot (\".\"), or \"@\" symbols.\n",
    "\n",
    "    pattern=r'\\w+[\\w.\\-_@]*\\w+'   \n",
    "    tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "    #print(len(tokens))\n",
    "    #print (tokens)\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # then find the POS tag of each word\n",
    "    # tagged_token is a list of (word, pos_tag)\n",
    "    tagged_tokens= nltk.pos_tag(tokens)\n",
    "    stop_words = stopwords.words('english')\n",
    "    def get_wordnet_pos(pos_tag):\n",
    "        \n",
    "        # if pos tag starts with 'J'\n",
    "        if pos_tag.startswith('J'):\n",
    "            # return wordnet tag \"ADJ\"\n",
    "            return wordnet.ADJ\n",
    "    \n",
    "        # if pos tag starts with 'V'\n",
    "        elif pos_tag.startswith('V'):\n",
    "            # return wordnet tag \"VERB\"\n",
    "            return wordnet.VERB\n",
    "    \n",
    "        # if pos tag starts with 'N'\n",
    "        elif pos_tag.startswith('N'):\n",
    "            # return wordnet tag \"NOUN\"\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # be default, return wordnet tag \"NOUN\"\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    lemmatized_words=[wordnet_lemmatizer.lemmatize\\\n",
    "          (word, get_wordnet_pos(tag)) \\\n",
    "          # tagged_tokens is a list of tuples (word, tag)\n",
    "          for (word, tag) in tagged_tokens \\\n",
    "          # remove stop words\n",
    "          if word not in stop_words and \\\n",
    "          # remove punctuations\n",
    "          word not in string.punctuation]\n",
    "    #print(lemmatized_words)\n",
    "    word_dist=nltk.FreqDist(lemmatized_words)\n",
    "    #token_count\n",
    "    return word_dist\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# Test Q1\n",
    "    text='''contact Yahoo! at \"http://login.yahoo.com\", select forgot\n",
    "    your password. If that fails to reset, contact Yahoo! at\n",
    "    their password department 408-349-1572 -- Can't promise\n",
    "    their phone department will fix, but they'll know where to\n",
    "    go next. Corporate emails from Yahoo! don't come from\n",
    "    their free mail system address space. Webmaster@yahoo.com\n",
    "    is not a corporate email address.'''\n",
    "    print(\"Test Q1\")\n",
    "    for key, value in tokenize(text).items():\n",
    "        print(key, value)\n",
    "# You should get the result look like :\n",
    "# contact 2 yahoo 3 http 1 login.yahoo.com 1\n",
    "# select 1 forget 1 password 2 fail 1\n",
    "# reset 1 department 2 408-349-1572 1 promise 1\n",
    "# phone 1 fix 1 know 1 go 1\n",
    "# next 1 corporate 2 email 2 come 1\n",
    "# free 1 mail 1 system 1 address 2\n",
    "# space 1 webmaster@yahoo.com 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"qa.csv\", header=0)\n",
    "#data['question']\n",
    "#doc_id=51\n",
    "#len(data['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"question\"].values.tolist()\n",
    "docs=' '.join(data[\"question\"].values.tolist())\n",
    "#question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"question\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "docs_tokens={idx:tokenize(doc) \\\n",
    "             for idx,doc in enumerate(data[\"question\"].values.tolist())}\n",
    "    # step 3. get document-term matrix\n",
    "dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "dtm=dtm.fillna(0)\n",
    "      \n",
    "    # step 4. get normalized term frequency (tf) matrix        \n",
    "tf=dtm.values\n",
    "doc_len=tf.sum(axis=1)\n",
    "tf=np.divide(tf.T, doc_len).T\n",
    "    \n",
    "    # step 5. get idf\n",
    "df=np.where(tf>0,1,0)\n",
    "    #idf=np.log(np.divide(len(docs), \\\n",
    "    #    np.sum(df, axis=0)))+1\n",
    "\n",
    "smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "smoothed_tf_idf=tf*smoothed_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    398\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m                                 \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__repr__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m                             \u001b[1;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[1;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 695\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36marray_repr\u001b[1;34m(arr, max_line_width, precision, suppress_small)\u001b[0m\n\u001b[0;32m   1429\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m         lst = array2string(arr, max_line_width, precision, suppress_small,\n\u001b[1;32m-> 1431\u001b[1;33m                            ', ', prefix, suffix=suffix)\n\u001b[0m\u001b[0;32m   1432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# show zero-length shape unless it is (0,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m         \u001b[0mlst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"[], shape=%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36marray2string\u001b[1;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, **kwarg)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"[]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_array2string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mrepr_running\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                 \u001b[0mrepr_running\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36m_array2string\u001b[1;34m(a, options, separator, prefix)\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'threshold'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m         \u001b[0msummary_insert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"...\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_leading_trailing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'edgeitems'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-a5dd17c798c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdocs_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m              \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "docs_tokens={idx:tokenize(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates tf_idf matrix from the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample text for analysis\n",
    "\n",
    "news=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]\n",
    "text=\". \".join(news).lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_matching_doc_id = None\n",
    "    similarity = None\n",
    "    # add your code here\n",
    "    #  process all documents to get list of token list\n",
    "    docs_tokens={idx:tokenize(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "\n",
    "    # get document-term matrix\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "      \n",
    "    #  get normalized term frequency (tf) matrix        \n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1)\n",
    "    tf=np.divide(tf.T, doc_len).T\n",
    "    \n",
    "    # get idf\n",
    "    df=np.where(tf>0,1,0)\n",
    "    #idf=np.log(np.divide(len(docs), \\\n",
    "    #    np.sum(df, axis=0)))+1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    # calculate cosince distance of every pair of documents \n",
    "    # convert the distance object into a square matrix form\n",
    "    # similarity is 1-distance\n",
    "    similarity=1-distance.squareform\\\n",
    "    (distance.pdist(smoothed_tf_idf, 'cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]\n",
    "text='. '.join(news).lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# library for normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# numpy is the package for matrix caculation\n",
    "import numpy as np  \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "docs=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokens from each documents\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Q1\n",
    "def tokenize(text):\n",
    "    #token_count = None\n",
    "    # add your code here\n",
    "    #convert the string to lowercase\n",
    "    #text.lower()\n",
    "    #Each token has at least two characters.\n",
    "    #The first/last character can only be a letter (i.e. a-z) or a number (0-9)\n",
    "    #In the middle, there are 0 or more characters, which can only be letters (a-z),\n",
    "    #numbers (0-9), hyphens (\"-\"), underscores (\"_\"), dot (\".\"), or \"@\" symbols.\n",
    "\n",
    "    pattern=r'\\w+[\\w.\\-_@]*\\w+'   \n",
    "    tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "    #print(len(tokens))\n",
    "    #print (tokens)\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # then find the POS tag of each word\n",
    "    # tagged_token is a list of (word, pos_tag)\n",
    "    tagged_tokens= nltk.pos_tag(tokens)\n",
    "    stop_words = stopwords.words('english')\n",
    "    def get_wordnet_pos(pos_tag):\n",
    "        \n",
    "        # if pos tag starts with 'J'\n",
    "        if pos_tag.startswith('J'):\n",
    "            # return wordnet tag \"ADJ\"\n",
    "            return wordnet.ADJ\n",
    "    \n",
    "        # if pos tag starts with 'V'\n",
    "        elif pos_tag.startswith('V'):\n",
    "            # return wordnet tag \"VERB\"\n",
    "            return wordnet.VERB\n",
    "    \n",
    "        # if pos tag starts with 'N'\n",
    "        elif pos_tag.startswith('N'):\n",
    "            # return wordnet tag \"NOUN\"\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # be default, return wordnet tag \"NOUN\"\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    lemmatized_words=[wordnet_lemmatizer.lemmatize\\\n",
    "          (word, get_wordnet_pos(tag)) \\\n",
    "          # tagged_tokens is a list of tuples (word, tag)\n",
    "          for (word, tag) in tagged_tokens \\\n",
    "          # remove stop words\n",
    "          if word not in stop_words and \\\n",
    "          # remove punctuations\n",
    "          word not in string.punctuation]\n",
    "    #print(lemmatized_words)\n",
    "    word_dist=nltk.FreqDist(lemmatized_words)\n",
    "    #token_count\n",
    "    return word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"qa.csv\", header=0)\n",
    "docs=data[\"question\"].values.tolist()\n",
    "docs_tokens={idx:tokenize(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "#print(docs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dtm=pd.DataFrame.from_dict(docs_tokens, \\\n",
    "                           orient=\"index\" )\n",
    "dtm=dtm.fillna(0)\n",
    "#dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-ee8431a996af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# sum the value of each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdoc_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# divide dtm matrix by the doc length matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36marray_str\u001b[1;34m(a, max_line_width, precision, suppress_small)\u001b[0m\n\u001b[0;32m   1502\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_guarded_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1504\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray2string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_line_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuppress_small\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mset_string_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36marray2string\u001b[1;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, **kwarg)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"[]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_array2string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mrepr_running\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                 \u001b[0mrepr_running\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36m_array2string\u001b[1;34m(a, options, separator, prefix)\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'threshold'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m         \u001b[0msummary_insert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"...\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_leading_trailing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'edgeitems'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "# step 4. get normalized term frequency (tf) matrix\n",
    "\n",
    "# convert dtm to numpy arrays\n",
    "tf=dtm.values\n",
    "\n",
    "# sum the value of each row\n",
    "doc_len=tf.sum(axis=1)\n",
    "print(doc_len)\n",
    "\n",
    "# divide dtm matrix by the doc length matrix\n",
    "tf=np.divide(tf, doc_len[:,None])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDF Matrix\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-f9afcd113348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nIDF Matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36marray_str\u001b[1;34m(a, max_line_width, precision, suppress_small)\u001b[0m\n\u001b[0;32m   1502\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_guarded_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1504\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray2string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_line_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuppress_small\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mset_string_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36marray2string\u001b[1;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, **kwarg)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"[]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_array2string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mrepr_running\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                 \u001b[0mrepr_running\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36m_array2string\u001b[1;34m(a, options, separator, prefix)\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'threshold'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m         \u001b[0msummary_insert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"...\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_leading_trailing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'edgeitems'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "# step 5. get idf\n",
    "\n",
    "# get document freqent\n",
    "df=np.where(tf>0,1,0)\n",
    "#df\n",
    "\n",
    "# get idf\n",
    "idf=np.log(np.divide(len(docs), \\\n",
    "        np.sum(df, axis=0)))+1\n",
    "print(\"\\nIDF Matrix\")\n",
    "print (idf)\n",
    "\n",
    "\n",
    "smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1\n",
    "print(\"\\nSmoothed IDF Matrix\")\n",
    "print(smoothed_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-777ab0a40f00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# step 6. get tf-idf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TF-IDF Matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtf_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalize' is not defined"
     ]
    }
   ],
   "source": [
    "# step 6. get tf-idf\n",
    "print(\"TF-IDF Matrix\")\n",
    "tf_idf=normalize(tf*idf)\n",
    "print(tf_idf)\n",
    "\n",
    "print(\"\\nSmoothed TF-IDF Matrix\")\n",
    "smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "print(smoothed_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zebra']\n",
      "['humour']\n",
      "['flat']\n",
      "['green']\n",
      "['newjersey']\n",
      "['bone']\n",
      "['sky']\n",
      "['documentation']\n",
      "['hospital']\n",
      "['empty']\n",
      "['speed']\n",
      "['color']\n",
      "['mistake']\n",
      "['besides']\n",
      "['gas']\n",
      "['sky']\n",
      "['tomorrow']\n",
      "['aafes']\n",
      "['snowy']\n",
      "['africa']\n",
      "['york']\n",
      "['respect']\n",
      "['infinite']\n",
      "['colour']\n",
      "['beside']\n",
      "['sneeze']\n",
      "['contact']\n",
      "['reach']\n",
      "['oklahoma']\n",
      "['betelgeuse']\n",
      "['13']\n",
      "['emergency']\n",
      "['help']\n",
      "['lamp']\n",
      "['sweat']\n",
      "['immigrant']\n",
      "['art']\n",
      "['out']\n",
      "['hit']\n",
      "['technology']\n",
      "['vote']\n",
      "['definition']\n",
      "['scientist']\n",
      "['seriously']\n",
      "['sky']\n",
      "['lab']\n",
      "['california']\n",
      "['possible']\n",
      "['classical']\n",
      "['rio']\n",
      "['house']\n",
      "['show']\n",
      "['chlorination']\n",
      "['gop']\n",
      "['great']\n",
      "['math-impared']\n",
      "['terror']\n",
      "['price']\n",
      "['blackhole']\n",
      "['evaporation']\n",
      "['inscribe']\n",
      "['iran']\n",
      "['sky']\n",
      "['you....yahoo']\n",
      "['history']\n",
      "['zero']\n",
      "['collision']\n",
      "['else']\n",
      "['date']\n",
      "['week']\n",
      "['expert']\n",
      "['full']\n",
      "['lawyer']\n",
      "['size']\n",
      "['look']\n",
      "['509']\n",
      "['police']\n",
      "['portland']\n",
      "['jeb']\n",
      "['crude']\n",
      "['substitute']\n",
      "['san']\n",
      "['embassy']\n",
      "['entomologist']\n",
      "['sweet']\n",
      "['power']\n",
      "['primitive']\n",
      "['24']\n",
      "['far']\n",
      "['sue']\n",
      "['extraterrestial']\n",
      "['surface']\n",
      "['connection']\n",
      "['involve']\n",
      "['acid']\n",
      "['increased']\n",
      "['sky']\n",
      "['project']\n",
      "['commercail']\n",
      "['move']\n",
      "['dual']\n",
      "['custody']\n",
      "['molecule']\n",
      "['star']\n",
      "['age']\n",
      "['finger']\n",
      "['influence']\n",
      "['term']\n",
      "['end']\n",
      "['family']\n",
      "['edison']\n",
      "['religious']\n",
      "['free']\n",
      "['brazil']\n",
      "['3n']\n",
      "['clair']\n",
      "['far']\n",
      "['government']\n",
      "['certify']\n",
      "['spirillum']\n",
      "['washington']\n",
      "['democrat']\n",
      "['ohio']\n",
      "['time']\n",
      "['come']\n",
      "['year']\n",
      "['shellback']\n",
      "['core']\n",
      "['different']\n",
      "['bacteria']\n",
      "['side']\n",
      "['note']\n",
      "['kilogram']\n",
      "['century']\n",
      "['22cm']\n",
      "['decompose']\n",
      "['horse']\n",
      "['african']\n",
      "['wipe']\n",
      "['nonlinear']\n",
      "['arrest']\n",
      "['tax']\n",
      "['presidency']\n",
      "['erlang']\n",
      "['run']\n",
      "['sneaker']\n",
      "['bad']\n",
      "['search']\n",
      "['point']\n",
      "['anemia']\n",
      "['saturn']\n",
      "['teleportation']\n",
      "['bind']\n",
      "['los']\n",
      "['tend']\n",
      "['cat']\n",
      "['easily']\n",
      "['pick']\n",
      "['sky']\n",
      "['g.w']\n",
      "['uniform']\n",
      "['nuclear']\n",
      "['constitutional']\n",
      "['fresh']\n",
      "['cabinet']\n",
      "['inertia']\n",
      "['zealand']\n",
      "['hugo']\n",
      "['fast']\n",
      "['jfk']\n",
      "['master']\n",
      "['important']\n",
      "['dwi']\n",
      "['philippine']\n",
      "['blood']\n",
      "['anybody']\n",
      "['realy']\n",
      "['particle']\n",
      "['length']\n",
      "['weight']\n",
      "['turn']\n",
      "['prize']\n",
      "['evolution']\n",
      "['theory']\n",
      "['conflict']\n",
      "['black']\n",
      "['homework']\n",
      "['hydroxide']\n",
      "['bond']\n",
      "['chinese']\n",
      "['143']\n",
      "['biology']\n",
      "['frost']\n",
      "['26']\n",
      "['ic']\n",
      "['terrost']\n",
      "['apply']\n",
      "['snow']\n",
      "['2006']\n",
      "['past']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7.2. Find the top three words \n",
    "# of each document by TF-IDF weight\n",
    "top=smoothed_tf_idf.argsort()[:,::-1][:,0:1]\n",
    "for row in top:\n",
    "    print([dtm.columns[x] for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_idf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-03bd100e51c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# convert the distance object into a square matrix form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# similarity is 1-distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msimilarity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquareform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cosine'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#print(len(similarity))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_idf' is not defined"
     ]
    }
   ],
   "source": [
    "# package to calculate distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# calculate cosince distance of every pair of documents \n",
    "# convert the distance object into a square matrix form\n",
    "# similarity is 1-distance\n",
    "similarity=1-distance.squareform\\\n",
    "(distance.pdist(tf_idf, 'cosine'))\n",
    "similarity\n",
    "#print(len(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why the sky is blue?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-8a2bba0385d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'similarity' is not defined"
     ]
    }
   ],
   "source": [
    "doc_id=15\n",
    "print(data[\"question\"].iloc[doc_id])\n",
    "np.argsort(similarity)[:,::-1][0,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.argsort(similarity)[:,::-1][0,0:2]\n",
    "#print(np.argsort(similarity))\n",
    "#for idx, doc in enumerate(docs):\n",
    "    #print(idx,doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"qa.csv\", header=0)\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"qa.csv\", header=0)\n",
    "docs_tokens={idx:tokenize(doc) \\\n",
    "             for idx,doc in enumerate(data[\"question\"].values.tolist())}\n",
    "#docs_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: FreqDist({'zebra': 1, 'stripe': 1}),\n",
       " 1: FreqDist({'animal': 1, 'sense': 1, 'humour': 1}),\n",
       " 2: FreqDist({'universe': 1, 'flat': 1}),\n",
       " 3: FreqDist({'u.s': 1, 'green': 1, 'card': 1, 'procedure': 1, 'labor': 1, 'certificate': 1, 'approval': 1, 'much': 1, 'time': 1, 'take': 1}),\n",
       " 4: FreqDist({'motor': 1, 'vehicle': 1, 'agency': 1, 'central': 1, 'newjersey': 1}),\n",
       " 5: FreqDist({'cancer': 1, 'type': 1, 'make': 1, 'metastasis': 1, 'bone': 1}),\n",
       " 6: FreqDist({'sky': 1, 'blue': 1}),\n",
       " 7: FreqDist({'foreignerin': 1, 'u': 1, 'temporarily': 1, 'drive': 1, 'foreign': 1, 'license': 1, 'need': 1, 'documentation': 1}),\n",
       " 8: FreqDist({'often': 1, 'president': 1, 'visit': 1, 'bethesda': 1, 'naval': 1, 'hospital': 1}),\n",
       " 9: FreqDist({'body': 1, 'water': 1, 'yukon': 1, 'river': 1, 'empty': 1}),\n",
       " 10: FreqDist({'speed': 2, 'life': 1, 'include': 1}),\n",
       " 11: FreqDist({'color': 1, 'water': 1, 'really': 1}),\n",
       " 12: FreqDist({'bright': 1, 'planet': 1, 'often': 1, 'mistake': 1, 'ufo': 1}),\n",
       " 13: FreqDist({'would': 1, 'good': 1, 'country': 1, 'live': 1, 'besides': 1, 'united': 1, 'state': 1}),\n",
       " 14: FreqDist({'gas': 2, 'milage': 1, 'well': 1, 'cold': 1, 'weather.does': 1, 'expand': 1, 'give': 1}),\n",
       " 15: FreqDist({'sky': 1, 'blue': 1}),\n",
       " 16: FreqDist({'chance': 1, 'wake': 1, 'tomorrow': 1, 'democratic': 1, 'president': 1}),\n",
       " 17: FreqDist({'aafes': 1, 'stand': 1}),\n",
       " 18: FreqDist({'salt': 1, 'spread': 1, 'snowy': 1, 'day': 1}),\n",
       " 19: FreqDist({'current': 1, 'president': 1, 'south': 1, 'africa': 1}),\n",
       " 20: FreqDist({'new': 1, 'york': 1, 'state': 1, 'iep': 1, 'individualized': 1, 'education': 1, 'program': 1, 'diploma': 1, 'accept': 1, 'army': 1}),\n",
       " 21: FreqDist({'think': 1, 'u': 1, 'ever': 1, 'recover': 1, 'bad': 1, 'foreign': 1, 'politics': 1, 'regain': 1, 'international': 1, 'respect': 1}),\n",
       " 22: FreqDist({'space': 1, 'infinite': 1}),\n",
       " 23: FreqDist({'animal': 1, 'see': 1, 'colour': 1}),\n",
       " 24: FreqDist({'really': 1, 'provoke': 1, 'cancer': 1, 'beside': 1, 'smoking': 1}),\n",
       " 25: FreqDist({'sneeze': 1, 'everytime': 1, 'see': 1, 'bright': 1, 'light': 1}),\n",
       " 26: FreqDist({'contact': 1, 'et': 1, 'yet': 1}),\n",
       " 27: FreqDist({'number': 1, 'x-ray': 1, 'reach': 1, 'film': 1, 'high': 1}),\n",
       " 28: FreqDist({'goofy': 1, 'law': 1, 'oklahoma': 1}),\n",
       " 29: FreqDist({'betelgeuse': 1, 'get': 1, 'name': 1}),\n",
       " 30: FreqDist({'13': 1, 'seer': 1, 'energy': 1, 'mandate': 1}),\n",
       " 31: FreqDist({'emergency': 1, 'room': 1, 'doctor': 1, 'get': 1, 'pay': 1, 'lot': 1}),\n",
       " 32: FreqDist({'id': 1, 'card': 1, 'help': 1, 'hinder': 1, 'fraudsters': 1}),\n",
       " 33: FreqDist({'dark': 1, 'room': 1, 'candle': 1, 'wood': 1, 'stove': 1, 'gas': 1, 'lamp': 1}),\n",
       " 34: FreqDist({'stink': 1, 'sweat': 1}),\n",
       " 35: FreqDist({'illegal': 1, 'immigrant': 1, 'legal': 1, 'right': 1}),\n",
       " 36: FreqDist({'art': 1, 'skuldudgery': 1}),\n",
       " 37: FreqDist({'out': 1, 'space': 1, 'consist': 1}),\n",
       " 38: FreqDist({'bad': 1, 'earthquake': 1, 'hit': 1, 'ever': 1}),\n",
       " 39: FreqDist({'concept': 1, 'spider-man': 1, 'real': 1, 'technology': 1, 'genetic': 1, 'engineering': 1, 'gene': 1, 'splicing': 1}),\n",
       " 40: FreqDist({'would': 1, 'u': 1, 'constitution': 1, 'change': 1, 'admendment': 1, 'receive': 1, 'popular': 1, 'vote': 1}),\n",
       " 41: FreqDist({'definition': 1, 'vacuole': 1}),\n",
       " 42: FreqDist({'possible': 1, 'scientist': 1, 'determine': 1, 'helium': 1, 'present': 1, 'sun': 1}),\n",
       " 43: FreqDist({'war': 1, 'mean': 1, 'seriously': 1}),\n",
       " 44: FreqDist({'sky': 1, 'blue': 1}),\n",
       " 45: FreqDist({'believe': 1, 'possible': 1, 'alien': 1, 'extra': 1, 'terrestrials': 1, 'genetically': 1, 'engineer': 1, 'human': 1, 'lab': 1}),\n",
       " 46: FreqDist({'arnold': 1, 'california': 1, 'governor': 1, 'stop': 1, 'last': 1, 'night': 1, 'execution': 1}),\n",
       " 47: FreqDist({'possible': 1, 'travel': 1, 'time': 1}),\n",
       " 48: FreqDist({'classical': 1, 'objection': 1, 'modern': 1, 'physic': 1}),\n",
       " 49: FreqDist({'big': 1, 'rio': 1, 'de': 1, 'la': 1, 'plata': 1, 'delta': 1}),\n",
       " 50: FreqDist({'fillibusters': 1, 'happen': 1, 'senate': 1, 'house': 1, 'rep': 1}),\n",
       " 51: FreqDist({'moon': 1, 'show': 1, 'face': 1}),\n",
       " 52: FreqDist({'make': 1, 'right': 1, 'chlorination': 1, 'use': 1, 'phosphorous': 1, 'oxychloride': 1}),\n",
       " 53: FreqDist({'gop': 1, 'stand': 1}),\n",
       " 54: FreqDist({'think': 1, 'man': 1, 'great': 1, 'invention': 1}),\n",
       " 55: FreqDist({'math': 1, 'complicate': 1, 'math-impared': 1}),\n",
       " 56: FreqDist({'war': 1, 'terror': 1}),\n",
       " 57: FreqDist({'formula': 1, 'choose': 1, 'stop': 1, 'check': 1, 'price': 1, 'good': 1}),\n",
       " 58: FreqDist({'blackhole': 1}),\n",
       " 59: FreqDist({'example': 1, 'evaporation': 1}),\n",
       " 60: FreqDist({'inscribe': 1, 'reular': 1, 'square': 1, 'circle': 1}),\n",
       " 61: FreqDist({'think': 1, 'iran': 1, 'leader': 1}),\n",
       " 62: FreqDist({'sky': 1, 'blue': 1}),\n",
       " 63: FreqDist({'you....yahoo': 1}),\n",
       " 64: FreqDist({'explain': 1, 'manifest': 1, 'destiny': 1, 'give': 1, 'example': 1, 'u': 1, 'history': 1}),\n",
       " 65: FreqDist({'sense': 1, 'make': 1, 'factorial': 1, 'zero': 1, 'one': 1}),\n",
       " 66: FreqDist({'collision': 2, 'calculate': 1, 'force': 1, 'duration': 1}),\n",
       " 67: FreqDist({'anyone': 1, 'else': 1, 'sick': 1, 'political': 1, 'bull': 1, 'thats': 1, 'go': 1}),\n",
       " 68: FreqDist({'much': 1, 'money': 1, 'u': 1, 'spend': 1, 'war': 1, 'iraq': 1, 'date': 1}),\n",
       " 69: FreqDist({'change': 1, 'mind': 1, 'war': 1, 'iraq': 1, 'last': 1, 'day': 1, 'week': 1}),\n",
       " 70: FreqDist({'call': 1, 'ye': 1, 'math': 1, 'expert': 1}),\n",
       " 71: FreqDist({'next': 1, 'full': 1, 'moon': 1}),\n",
       " 72: FreqDist({'anyone': 1, 'know': 1, 'real': 1, 'lawyer': 1, 'missouri': 1}),\n",
       " 73: FreqDist({'size': 1, 'eliptical': 1, 'galaxy': 1}),\n",
       " 74: FreqDist({'start': 1, 'look': 1, 'alternative': 1, 'energy': 1, 'source': 1, 'decade': 1}),\n",
       " 75: FreqDist({'non': 1, 'profit': 1, '509': 1}),\n",
       " 76: FreqDist({'police': 2, 'anyone': 1, 'know': 1, 'web': 1, 'site': 1, 'san': 1, 'fransisco': 1, 'officer': 1, 'make': 1, 'bad': 1, ...}),\n",
       " 77: FreqDist({'time': 1, 'difference': 1, 'iraq': 1, 'portland': 1, 'oregon': 1, 'usa': 1}),\n",
       " 78: FreqDist({'jeb': 1, 'bush': 1, 'run': 1, 'president': 1, 'next': 1, 'election': 1}),\n",
       " 79: FreqDist({'crude': 1, 'oil': 1, 'form': 1}),\n",
       " 80: FreqDist({'way': 1, 'substitute': 1, 'sleep': 1}),\n",
       " 81: FreqDist({'distance': 1, 'earth': 1, 'san': 1}),\n",
       " 82: FreqDist({'want': 1, 'make': 1, 'invitation': 1, 'letter': 1, 'council': 1, 'embassy': 1}),\n",
       " 83: FreqDist({'entomologist': 1, 'note': 1, 'past': 1, 'year': 1, 'local': 1, 'forest': 1, 'preserve': 1, 'population': 1, 'bee': 1, 'decrease': 1}),\n",
       " 84: FreqDist({'sweet': 1, 'flavor': 1, 'good': 1}),\n",
       " 85: FreqDist({'next': 1, 'super': 1, 'power': 1}),\n",
       " 86: FreqDist({'think': 1, 'god': 1, 'may': 1, 'alien': 1, 'come': 1, 'earth': 1, 'civilization': 1, 'primitive': 1}),\n",
       " 87: FreqDist({'much': 1, 'dose': 1, 'cost': 1, 'run': 1, 'pc': 1, '24': 1}),\n",
       " 88: FreqDist({'far': 1, 'earth': 1, 'sun': 1}),\n",
       " 89: FreqDist({'sue': 1, 'case': 1, 'air': 1, 'bag': 1, 'deploy': 1, 'car': 1, 'accident': 1}),\n",
       " 90: FreqDist({'believe': 1, 'ufo': 1, 'extraterrestial': 1, 'visit': 1, 'planet': 1}),\n",
       " 91: FreqDist({'measure': 1, 'much': 1, 'surface': 1, 'object': 1}),\n",
       " 92: FreqDist({'connection': 1, 'phase': 1, 'moon': 1, 'earthquake': 1, 'volcano': 1}),\n",
       " 93: FreqDist({'hazard': 1, 'action': 1, 'law': 1, 'mean': 1, 'live': 1, 'monroe': 1, 'county': 1, 'michigan': 1, 'involve': 1, 'accident': 1}),\n",
       " 94: FreqDist({'acid': 2, 'functional': 1, 'group': 1, 'butanoic': 1, 'organic': 1, 'blank': 1}),\n",
       " 95: FreqDist({'treatment': 1, 'increased': 1, 'level': 1, 'floater': 1, 'eye': 1}),\n",
       " 96: FreqDist({'sky': 1, 'blue': 1}),\n",
       " 97: FreqDist({'project': 2, 'want': 1, 'science': 1, 'diesel': 1, 'engine': 1, 'site': 1, 'introduce': 1, 'new': 1, 'field': 1}),\n",
       " 98: FreqDist({'rain': 1, 'garden': 1, 'filter': 1, 'bioretention': 1, 'use': 1, 'commercail': 1, 'purpose': 1, 'water': 1, 'treatment': 1, 'plant': 1}),\n",
       " 99: FreqDist({'anyone': 1, 'know': 1, 'move': 1, 'thing': 1, 'mind': 1, 'story': 1, 'might': 1, 'hear': 1, 'achieve': 1}),\n",
       " 100: FreqDist({'dual': 1, 'citizenship': 1}),\n",
       " 101: FreqDist({'child': 1, 'custody': 1, 'law': 1, 'without': 1, 'marriage': 1}),\n",
       " 102: FreqDist({'atp': 1, 'gtp': 1, 'case': 1, 'energy': 1, 'give': 1, 'molecule': 1}),\n",
       " 103: FreqDist({'name': 1, 'near': 1, 'star': 1}),\n",
       " 104: FreqDist({'right': 1, 'american': 1, 'citizen': 1, 'age': 1, '18': 1}),\n",
       " 105: FreqDist({'finger': 1, 'nail': 1, 'continue': 1, 'grow': 1, 'die': 1}),\n",
       " 106: FreqDist({'time': 1, 'period': 1, 'ancient': 1, 'greece': 1, 'influence': 1, 'world': 1}),\n",
       " 107: FreqDist({'asian': 1, 'replace': 1, 'oriental': 1, 'politically': 1, 'correct': 1, 'term': 1, 'see': 1}),\n",
       " 108: FreqDist({'universe': 1, 'end': 1}),\n",
       " 109: FreqDist({'canadian': 1, 'family': 1, 'law': 1}),\n",
       " 110: FreqDist({'edison': 1}),\n",
       " 111: FreqDist({'religious': 1, 'breakdown': 1, 'population': 1, 'united': 1, 'state': 1}),\n",
       " 112: FreqDist({'free': 1, 'state': 1, 'project': 1}),\n",
       " 113: FreqDist({'capital': 1, 'brazil': 1}),\n",
       " 114: FreqDist({'3n': 1, '16': 1, '8n': 1, '29': 1}),\n",
       " 115: FreqDist({'uptinson': 1, 'clair': 1}),\n",
       " 116: FreqDist({'far': 1, 'earth': 1, 'sun': 1}),\n",
       " 117: FreqDist({'describe': 1, 'democratic': 1, 'government': 1}),\n",
       " 118: FreqDist({'get': 1, 'certify': 1, 'marry': 1, 'people': 1}),\n",
       " 119: FreqDist({'bacteria': 1, 'spirillum': 1}),\n",
       " 120: FreqDist({'pull': 1, 'washington': 1}),\n",
       " 121: FreqDist({'2008': 1, 'democrat': 1, 'nominee': 1, 'president': 1}),\n",
       " 122: FreqDist({'capital': 1, 'ohio': 1}),\n",
       " 123: FreqDist({'time': 1}),\n",
       " 124: FreqDist({'come': 2, 'first': 1, 'god': 1, 'dinosaurs': 1, 'bible': 1, 'talk': 1, 'dinosaur': 1}),\n",
       " 125: FreqDist({'new': 1, 'year': 1}),\n",
       " 126: FreqDist({'whats': 1, 'golden': 1, 'shellback': 1}),\n",
       " 127: FreqDist({'hot': 1, 'inner': 1, 'core': 1}),\n",
       " 128: FreqDist({'different': 2, 'lenght': 1, 'dna': 1, 'organism': 1}),\n",
       " 129: FreqDist({'would': 1, 'happen': 1, 'bacteria': 1, 'body': 1}),\n",
       " 130: FreqDist({'perimeter': 1, 'square': 1, 'side': 1, 'measure': 1, '45cm': 1}),\n",
       " 131: FreqDist({'note': 2, 'rational': 1, 'frequency': 1}),\n",
       " 132: FreqDist({'many': 1, 'kilogram': 1, 'moon': 1, 'weigh': 1}),\n",
       " 133: FreqDist({'america': 1, 'keep': 1, 'become': 1, 'dominate': 1, 'china': 1, '21st': 1, 'century': 1}),\n",
       " 134: FreqDist({'area': 1, 'circle': 1, 'diameter': 1, '22cm': 1}),\n",
       " 135: FreqDist({'person': 1, 'bury': 1, 'space': 1, 'would': 1, 'body': 1, 'decompose': 1}),\n",
       " 136: FreqDist({'horse': 1, 'sleep': 1, 'lie': 1}),\n",
       " 137: FreqDist({'many': 1, 'african': 1, 'dream': 1, 'america': 1}),\n",
       " 138: FreqDist({'would': 1, 'best': 1, 'way': 1, 'wipe': 1, 'crime': 1}),\n",
       " 139: FreqDist({'nonlinear': 1, 'programming': 1}),\n",
       " 140: FreqDist({'find': 1, 'person': 1, 'arrest': 1, 'warrant': 1, 'issue': 1}),\n",
       " 141: FreqDist({'pay': 1, 'child': 1, 'support': 1, 'ex': 1, 'girl': 1, 'friend': 1, 'daughter': 1, 'legally': 1, 'claim': 1, 'tax': 1}),\n",
       " 142: FreqDist({'republicans': 1, 'win': 1, 'presidency': 1, '2008': 1}),\n",
       " 143: FreqDist({'erlang': 1}),\n",
       " 144: FreqDist({'run': 2, 'car': 1, 'speed': 1, 'period': 1, 'time': 1, 'distance': 1, 'need': 1, 'equation': 1}),\n",
       " 145: FreqDist({'sneaker': 1, 'make': 1}),\n",
       " 146: FreqDist({'bad': 1, 'president': 1}),\n",
       " 147: FreqDist({'search': 1, 'animated': 1, 'describtion': 1, 'race': 1, 'car': 1, 'engine': 1, 'work': 1}),\n",
       " 148: FreqDist({'southern': 1, 'point': 1, 'america': 1}),\n",
       " 149: FreqDist({'people': 1, 'obsessively': 1, 'eat': 1, 'ice': 1, 'iron': 1, 'deficiency': 1, 'anemia': 1}),\n",
       " 150: FreqDist({'long': 1, 'saturn': 1, 'go': 1, 'around': 1, 'sun': 1}),\n",
       " 151: FreqDist({'much': 1, 'long': 1, 'take': 1, 'invent': 1, 'teleportation': 1}),\n",
       " 152: FreqDist({'bind': 1, 'energy': 1, 'hydrogen': 1}),\n",
       " 153: FreqDist({'find': 1, 'los': 1, 'angeles': 1, 'county': 1, 'public': 1, 'property': 1, 'record': 1}),\n",
       " 154: FreqDist({'object': 1, 'spoon': 1, 'reflect': 1, 'tend': 1, 'appear': 1, 'upside': 1}),\n",
       " 155: FreqDist({'large': 1, 'cat': 1}),\n",
       " 156: FreqDist({'multiply': 1, 'big': 1, 'number': 1, 'easily': 1}),\n",
       " 157: FreqDist({'istr': 1, 'trick': 1, 'pick': 1, 'chair': 1, 'woman': 1, 'men': 1}),\n",
       " 158: FreqDist({'sky': 1, 'blue': 1}),\n",
       " 159: FreqDist({'anyone': 1, 'explain': 1, 'g.w': 1, 'bush': 1, 'impeach': 1, 'yet': 1}),\n",
       " 160: FreqDist({'calculate': 1, 'deflection': 1, 'rectangular': 1, 'slab': 1, 'subject': 1, 'uniform': 1, 'pressure': 1}),\n",
       " 161: FreqDist({'country': 1, 'nuclear': 1, 'weapon': 1}),\n",
       " 162: FreqDist({'give': 1, 'one': 1, 'constitutional': 1, 'right': 1, 'would': 1}),\n",
       " 163: FreqDist({'neon': 1, 'less': 1, 'dense': 1, 'fresh': 1, 'water': 1, 'room': 1, 'temperature': 1}),\n",
       " 164: FreqDist({'people': 1, 'name': 1, 'current': 1, 'cabinet': 1, 'position': 1}),\n",
       " 165: FreqDist({'think': 1, 'mass': 1, 'inertia': 1, 'come': 1}),\n",
       " 166: FreqDist({'become': 1, 'citizen': 1, 'new': 1, 'zealand': 1}),\n",
       " 167: FreqDist({'hugo': 1, 'chavez': 1, 'popular': 1}),\n",
       " 168: FreqDist({'fast': 1, 'earth': 1, 'rotate': 1, 'axis': 1}),\n",
       " 169: FreqDist({'jfk': 1, 'effect': 1, 'economy': 1}),\n",
       " 170: FreqDist({'president': 1, 'bush': 1, 'get': 1, 'master': 1, 'degree': 1}),\n",
       " 171: FreqDist({'economic': 1, 'debate': 1, 'become': 1, 'important': 1}),\n",
       " 172: FreqDist({'difference': 1, 'duo': 1, 'dwi': 1}),\n",
       " 173: FreqDist({'need': 1, 'get': 1, 'visa': 1, 'someone': 1, 'live': 1, 'philippine': 1, 'come': 1, 'marry': 1, 'start': 1}),\n",
       " 174: FreqDist({'blood': 2, 'type': 2, 'people': 1, 'rh': 1, 'negative': 1, 'also': 1, 'classify': 1, 'abo': 1, 'system': 1}),\n",
       " 175: FreqDist({'anybody': 1, 'know': 1, 'chapman-kolmogorov': 1, 'equation': 1}),\n",
       " 176: FreqDist({'realy': 1, 'believe': 1, 'life': 1, 'form': 1, 'space': 1, 'dont': 1}),\n",
       " 177: FreqDist({'even': 1, 'though': 1, 'quark': 1, 'small': 1, 'indivisible': 1, 'particle': 1, 'make': 1, 'see': 1, 'detail': 1, 'cmplt': 1, ...}),\n",
       " 178: FreqDist({'length': 1, 'duration': 1, 'one': 1, 'revolution': 1, 'sun': 1, 'around': 1, 'center': 1, 'milkyway': 1, 'galixy': 1}),\n",
       " 179: FreqDist({'weight': 1, 'small': 1, 'virus': 1}),\n",
       " 180: FreqDist({'travel': 1, 'vehicle': 1, 'speed': 1, 'light': 1, 'turn': 1, 'headlight': 1, 'would': 1, 'work': 1}),\n",
       " 181: FreqDist({'sudoku': 1, 'puzzle': 1, 'competition': 1, 'prize': 1}),\n",
       " 182: FreqDist({'tha': 1, 'real': 1, 'thing': 1, 'keep': 1, 'life': 1, 'evolution': 1, 'earth': 1, 'think': 1, 'moon': 1, 'water': 1}),\n",
       " 183: FreqDist({'perceptual': 1, 'control': 1, 'theory': 1}),\n",
       " 184: FreqDist({'right': 1, 'israeli-palestinian': 1, 'conflict': 1}),\n",
       " 185: FreqDist({'black': 1, 'ice': 1}),\n",
       " 186: FreqDist({'someone': 1, 'take': 1, 'picture': 1, 'homework': 1, 'assignment': 1, 'sell': 1, 'legal': 1}),\n",
       " 187: FreqDist({'product': 1, 'neutralization': 1, 'reaction': 1, 'hydrochloric': 1, 'acid': 1, 'magnesium': 1, 'hydroxide': 1}),\n",
       " 188: FreqDist({'hydrogen': 1, 'bond': 1, 'form': 1}),\n",
       " 189: FreqDist({'marry': 1, 'chinese': 1, 'national': 1, 'best': 1, 'visa': 1, 'option': 1, 'travel': 1, 'return': 1, 'usa': 1}),\n",
       " 190: FreqDist({'eeuma': 1, '143': 1}),\n",
       " 191: FreqDist({'come': 1, 'chicken': 1, 'lay': 1, 'egg': 1, 'every': 1, 'day': 1, 'biology': 1, 'explanation': 1}),\n",
       " 192: FreqDist({'difference': 1, 'frost': 1, 'freeze': 1}),\n",
       " 193: FreqDist({'rain': 1, 'temperature': 1, '26': 1, 'degree': 1, 'happen': 1, 'right': 1, 'se': 1, 'idaho': 1}),\n",
       " 194: FreqDist({'explain': 1, 'ic': 1, 'fabrication': 1}),\n",
       " 195: FreqDist({'do': 1, 'protect': 1, 'terrost': 1, 'from........blowing': 1, 'sewer': 1, 'sysytems......anybody': 1}),\n",
       " 196: FreqDist({'sinusodial': 1, 'equation': 1, 'apply': 1, 'real': 1, 'world': 1, 'ex': 1, 'moon': 1, 'phase': 1}),\n",
       " 197: FreqDist({'difference': 1, 'ice': 1, 'snow': 1}),\n",
       " 198: FreqDist({'india': 1, 'cbse': 1, 'board': 1, 'exam': 1, 'datesheet': 1, '12th': 1, '2006': 1}),\n",
       " 199: FreqDist({'past': 1, 'space': 1})}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"qa.csv\", header=0)\n",
    "#data.head()\n",
    "docs=data[\"question\"].values.tolist()\n",
    "docs_tokens={idx:tokenize(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "docs_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
