{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "# package to calculate distance\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    #token_count = None\n",
    "    # add your code here\n",
    "    #convert the string to lowercase\n",
    "    #text.lower()\n",
    "    #Each token has at least two characters.\n",
    "    #The first/last character can only be a letter (i.e. a-z) or a number (0-9)\n",
    "    #In the middle, there are 0 or more characters, which can only be letters (a-z),\n",
    "    #numbers (0-9), hyphens (\"-\"), underscores (\"_\"), dot (\".\"), or \"@\" symbols.\n",
    "\n",
    "    pattern=r'\\w+[\\w\\-_\\.@]*\\w+'   \n",
    "    tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "    #print(len(tokens))\n",
    "    #print (tokens)\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # then find the POS tag of each word\n",
    "    # tagged_token is a list of (word, pos_tag)\n",
    "    tagged_tokens= nltk.pos_tag(tokens)\n",
    "    stop_words = stopwords.words('english')\n",
    "    def get_wordnet_pos(pos_tag):\n",
    "        \n",
    "        # if pos tag starts with 'J'\n",
    "        if pos_tag.startswith('J'):\n",
    "            # return wordnet tag \"ADJ\"\n",
    "            return wordnet.ADJ\n",
    "    \n",
    "        # if pos tag starts with 'V'\n",
    "        elif pos_tag.startswith('V'):\n",
    "            # return wordnet tag \"VERB\"\n",
    "            return wordnet.VERB\n",
    "    \n",
    "        # if pos tag starts with 'N'\n",
    "        elif pos_tag.startswith('N'):\n",
    "            # return wordnet tag \"NOUN\"\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # be default, return wordnet tag \"NOUN\"\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    lemmatized_words=[wordnet_lemmatizer.lemmatize\\\n",
    "          (word, get_wordnet_pos(tag)) \\\n",
    "          # tagged_tokens is a list of tuples (word, tag)\n",
    "          for (word, tag) in tagged_tokens \\\n",
    "          # remove stop words\n",
    "          if word not in stop_words and \\\n",
    "          # remove punctuations\n",
    "          word not in string.punctuation]\n",
    "    #print(lemmatized_words)\n",
    "    token_count=nltk.FreqDist(lemmatized_words)\n",
    "    #token_count\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"qa.csv\", header=0)\n",
    "docs=data[\"question\"].values.tolist()\n",
    "#  process all documents to get list of token list\n",
    "docs_tokens={idx:tokenize(doc) \\\n",
    "         for idx,doc in enumerate(docs)}\n",
    "\n",
    "# get document-term matrix\n",
    "dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "dtm=dtm.fillna(0)\n",
    "\n",
    "#  get normalized term frequency (tf) matrix        \n",
    "tf=dtm.values\n",
    "doc_len=tf.sum(axis=1)\n",
    "tf=np.divide(tf.T, doc_len).T\n",
    "\n",
    "# get idf\n",
    "df=np.where(tf>0,1,0)\n",
    "#idf=np.log(np.divide(len(docs), \\\n",
    "#    np.sum(df, axis=0)))+1\n",
    "\n",
    "smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "smoothed_tf_idf=tf*smoothed_idf\n",
    "\n",
    "# calculate cosince distance of every pair of documents \n",
    "# convert the distance object into a square matrix form\n",
    "# similarity is 1-distance\n",
    "similarity=1-distance.squareform\\\n",
    "(distance.pdist(smoothed_tf_idf, 'cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
