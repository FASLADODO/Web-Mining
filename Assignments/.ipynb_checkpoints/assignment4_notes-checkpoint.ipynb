{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COM-101   COMPUTERS\n",
      "COM-111   DATABASE\n",
      "COM-211   ALGORITHM\n",
      "MAT-103   STATISTICS learning\n",
      "MAT-102   STATISTICS\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"\"\"COM-101   COMPUTERS\n",
    "COM-111   DATABASE\n",
    "COM-211   ALGORITHM\n",
    "MAT-103   STATISTICS learning\n",
    "MAT-102   STATISTICS\"\"\"\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COM', 'COM', 'COM', 'COM']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['COM']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['STATISTICS']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['101']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['101', '103', '102']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['103']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.findall(\".\",text)\n",
    "re.findall(\"COM\",text)\n",
    "re.findall(\"^COM\",text)\n",
    "re.findall(\"STATISTICS$\",text)\n",
    "re.findall(\"101\",text)\n",
    "re.findall('10[123]',text)\n",
    "re.findall('10[^12]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COM-1', 'COM-111', 'COM-']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['COM-1', 'COM-111']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['COM-1', 'COM-1', 'COM-']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('COM-1*',text)\n",
    "re.findall('COM-1+',text)\n",
    "re.findall('COM-1?',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11', '11']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['111', '11']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['111']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('1{2}',text)\n",
    "re.findall('1{2,}',text)\n",
    "re.findall('1{3}',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101', '111', '211', '103', '102']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['learning']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['COM',\n",
       " 'COMPUTERS',\n",
       " 'COM',\n",
       " 'DATABASE',\n",
       " 'COM',\n",
       " 'ALGORITHM',\n",
       " 'MAT',\n",
       " 'STATISTICS',\n",
       " 'MAT',\n",
       " 'STATISTICS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[0-9]+',text)\n",
    "re.findall('[a-z]+',text)\n",
    "re.findall('[A-Z]+',text)\n",
    "#re.findall('[a-zA-Z0-9]+',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COM-',\n",
       " '   COMPUTERS\\nCOM-',\n",
       " '   DATABASE\\nCOM-',\n",
       " '   ALGORITHM\\nMAT-',\n",
       " '   STATISTICS learning\\nMAT-',\n",
       " '   STATISTICS']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['101', '103', '102']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[^0-9]+',text)\n",
    "re.findall('101|102|103',text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COM',\n",
       " '101',\n",
       " 'COMPUTERS',\n",
       " 'COM',\n",
       " '111',\n",
       " 'DATABASE',\n",
       " 'COM',\n",
       " '211',\n",
       " 'ALGORITHM',\n",
       " 'MAT',\n",
       " '103',\n",
       " 'STATISTICS',\n",
       " 'learning',\n",
       " 'MAT',\n",
       " '102',\n",
       " 'STATISTICS']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '   ',\n",
       " '\\n',\n",
       " '-',\n",
       " '   ',\n",
       " '\\n',\n",
       " '-',\n",
       " '   ',\n",
       " '\\n',\n",
       " '-',\n",
       " '   ',\n",
       " ' ',\n",
       " '\\n',\n",
       " '-',\n",
       " '   ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['M-',\n",
       " '1   ',\n",
       " 'S\\n',\n",
       " 'M-',\n",
       " '1   ',\n",
       " 'E\\n',\n",
       " 'M-',\n",
       " '1   ',\n",
       " 'M\\n',\n",
       " 'T-',\n",
       " '3   ',\n",
       " 'S ',\n",
       " 'g\\n',\n",
       " 'T-',\n",
       " '2   ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+',text) #word character\n",
    "#re.findall('[a-zA-Z0-9]+',text)\n",
    "re.findall('\\W+',text)#non-word character\n",
    "re.findall('\\w\\W+',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find cat!\n",
      "The cat cat\n"
     ]
    }
   ],
   "source": [
    "text=\"The cat catches a rat\"\n",
    "\n",
    "match= re.search(r'.*cat',text)\n",
    "if match:\n",
    "    print (\"find cat!\")\n",
    "    print (match.group())\n",
    "else:\n",
    "    print (\"not found!\")\n",
    "# greedy match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'cat']\n",
      "['The', 'cat', 'catches', 'a', 'rat']\n"
     ]
    }
   ],
   "source": [
    "# findall function\n",
    "match=re.findall(r'cat',text)\n",
    "print(match)\n",
    "# split function\n",
    "match=re.split(r'\\W+',text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CAT CATches a rat\n"
     ]
    }
   ],
   "source": [
    "# sub function\n",
    "match=re.sub(r'cat', 'CAT', text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T[A-Z] [A-Z] [A-Z] [A-Z] [A-Z]\n"
     ]
    }
   ],
   "source": [
    "match=re.sub(r'[a-z]+',r'[A-Z]',text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 're' has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-85b27f8e1a72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# question\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#how to capture the first cat but ignore the cat inside catch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'cat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 're' has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# question \n",
    "#how to capture the first cat but ignore the cat inside catch\n",
    "match=re.find(r'cat',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'catches', 'a', 'rat']\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "words=re.split(r'\\W+',text)\n",
    "print(words)\n",
    "for word in words:\n",
    "    if word=='cat':\n",
    "        print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'catches', 'a', 'rat']\n"
     ]
    }
   ],
   "source": [
    "words=re.findall(r'\\w+',text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issac Newton\n",
      "Issac Newton\n",
      "Issac Newton, Scientist\n",
      "[('Issac', 'Newton', 'Scientist')]\n",
      "[('Issac', 'Newton', 'Scientist')]\n",
      "[('Issac', 'Newton', 'Scientist')]\n",
      "['Issac Newton, Scientist']\n"
     ]
    }
   ],
   "source": [
    "m=re.match(r'\\w+ \\w+','Issac Newton, Scientist')\n",
    "print(m.group())\n",
    "print(m.group(0)) # whole text\n",
    "#print(m.group(1))# no such group. r'(\\w+) (\\w+)'\n",
    "#print(m.group(2))\n",
    "m=re.match(r'(\\w+) (\\w+)[\\,] (\\w+)', 'Issac Newton, Scientist')\n",
    "print(m.group())\n",
    "\n",
    "m=re.findall(r'(\\w{5})[\\W]+(\\w{6})[\\W]+(\\w{9})','Issac Newton, Scientist')\n",
    "#m=re.findall(r'(\\w{5})[\\W+](\\w{6})[\\W+](\\w{9})','Issac Newton, Scientist')\n",
    "#m=re.findall(r'(\\w+)[\\s+](\\w+)[,](\\w+)','Issac Newton, Scientist')\n",
    "print(m)\n",
    "m=re.findall(r'(\\w{5}) (\\w{6}), (\\w{9})','Issac Newton, Scientist')\n",
    "print(m)\n",
    "m=re.findall(r'(\\w{5})\\W+(\\w{6})\\W+(\\w{9})','Issac Newton, Scientist')\n",
    "print(m)\n",
    "m=re.findall(r'\\w{5}\\W+\\w{6}\\W+\\w{9}','Issac Newton, Scientist')\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201-959-5599\n",
      "['201-959-5599', '201-966-5599']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.2. find phone number\n",
    "text = \"201-959-5599 # This is Phone Number 201-966-5599\"\n",
    "\n",
    "phone=re.match(r'\\d{3}-\\d{3}-\\d{4}',text)\n",
    "print(phone.group())\n",
    "\n",
    "phone=re.findall(r'\\d{3}-\\d{3}-\\d{4}',text)\n",
    "print(phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['201.959-5599']\n",
      "['201.959-5599', '201-966-5599']\n",
      "[('201', '959', '5599'), ('201', '966', '5599')]\n"
     ]
    }
   ],
   "source": [
    "# How about phone numbers like 201.959.5599?\n",
    "text = \"201.959-5599 # This is Phone Number 201-966-5599\"\n",
    "\n",
    "a=re.findall(r'\\d{3}\\.\\d{3}-\\d{4}',text)\n",
    "print(a)\n",
    "phones2 = re.findall(r'\\d{3}[-\\.]\\d{3}[-\\.]\\d{4}', text)\n",
    "print(phones2)\n",
    "b=re.findall(r'(\\d{3})[-\\.](\\d{3})[-\\.](\\d{4})',text)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joe.doe@example1.com', 'abc-xyz@example2.edu']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.3. find email address\n",
    "text = \"email me at joe.doe@example1.com or at abc-xyz@example2.edu\"\n",
    "\n",
    "a=re.findall(r'[a-zA-Z0-9._-]+@[a-zA-Z0-9.-_]+',text)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('COM-101', 'COMPUTERS'), ('COM-111', 'DATABASE'), ('COM-211', 'ALGORITHM'), ('MAT-103', 'STATISTICS'), ('MAT-102', 'STATISTICS')]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.4. Extract course name and title as \n",
    "# [('COM-101', 'COMPUTERS'),\n",
    "#  ('COM-111', 'DATABASE'),\n",
    "#  ... ]\n",
    "\n",
    "text = '''COM-101   COMPUTERS\n",
    "COM-111   DATABASE\n",
    "COM-211   ALGORITHM\n",
    "MAT-103   STATISTICS learning\n",
    "MAT-102   STATISTICS'''\n",
    "\n",
    "a=re.findall(r'(\\w+-\\d+)\\s+(\\w+)',text)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''`strange days' chronicles the last two days of 1999 in los angeles. \n",
    " as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips. \n",
    " he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him. \n",
    " this film features good performances, impressive film-making technique and breath-taking crowd scenes. \n",
    " director kathryn bigelow knows her stuff and does not hesitate to use it. \n",
    " but as a whole, this is an unsatisfying movie. \n",
    " the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \n",
    " not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \n",
    " the film just ends up preachy, unexciting and uninvolving.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`strange days' chronicles the last two days of 1999 in los angeles. \\n as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips. \\n he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him. \\n this film features good performances, impressive film-making technique and breath-taking crowd scenes. \\n director kathryn bigelow knows her stuff and does not hesitate to use it. \\n but as a whole, this is an unsatisfying movie. \\n the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \\n not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \\n the film just ends up preachy, unexciting and uninvolving.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'strange', 'days', 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', 'lenny', 'nero', 'ralph', 'fiennes', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', 'he', 'pines', 'for', 'his', 'ex', 'girlfriend', 'faith', 'juliette', 'lewis', 'but', 'doesn', 't', 'notice', 'that', 'another', 'friend', 'mace', 'angela', 'bassett', 'really', 'cares', 'for', 'him', 'this', 'film', 'features', 'good', 'performances', 'impressive', 'film', 'making', 'technique', 'and', 'breath', 'taking', 'crowd', 'scenes', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', 'but', 'as', 'a', 'whole', 'this', 'is', 'an', 'unsatisfying', 'movie', 'the', 'problem', 'is', 'that', 'the', 'writers', 'james', 'cameron', 'and', 'jay', 'cocks', 'were', 'too', 'ambitious', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', 'thrills', 'and', 'drama', 'not', 'that', 'ambitious', 'film', 'making', 'should', 'be', 'discouraged', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', 'it', 'fails', 'badly', 'and', 'obviously', 'the', 'film', 'just', 'ends', 'up', 'preachy', 'unexciting', 'and', 'uninvolving', '']\n"
     ]
    }
   ],
   "source": [
    "tokens=re.split('\\W+',text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['`', 'strange', 'days', \"'\", 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', '.', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', ',', 'lenny', 'nero', '(', 'ralph', 'fiennes', ')', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', '.', 'he', 'pines', 'for', 'his', 'ex-girlfriend', ',', 'faith', '(', 'juliette', 'lewis', ')', ',', 'but', 'does', \"n't\", 'notice', 'that', 'another', 'friend', ',', 'mace', '(', 'angela', 'bassett', ')', 'really', 'cares', 'for', 'him', '.', 'this', 'film', 'features', 'good', 'performances', ',', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', '.', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', '.', 'but', 'as', 'a', 'whole', ',', 'this', 'is', 'an', 'unsatisfying', 'movie', '.', 'the', 'problem', 'is', 'that', 'the', 'writers', ',', 'james', 'cameron', 'and', 'jay', 'cocks', ',', 'were', 'too', 'ambitious', ',', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', ',', 'thrills', ',', 'and', 'drama', '.', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', ';', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', ',', 'it', 'fails', 'badly', 'and', 'obviously', '.', 'the', 'film', 'just', 'ends', 'up', 'preachy', ',', 'unexciting', 'and', 'uninvolving', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[token.strip(string.punctuation) for token in tokens]\n",
    "#print(tokens)\n",
    "tokens=[token.strip() for token in tokens if token.strip()!='']\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"strange days' chronicles the last two days of 1999 in los angeles.\", 'as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips.', \"he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him.\", 'this film features good performances, impressive film-making technique and breath-taking crowd scenes.', 'director kathryn bigelow knows her stuff and does not hesitate to use it.', 'but as a whole, this is an unsatisfying movie.', 'the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama.', 'not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously.', 'the film just ends up preachy, unexciting and uninvolving.']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.5 Use NLTK's regular expression tokenizer \n",
    "# to define sentences, i.e. \n",
    "# (1) starts with non-space character, \n",
    "# (2) contains any number of characters in the middle, \n",
    "#     as long as they are not \"!?.\"\n",
    "# (3) ends with !?.\n",
    "pattern= r'\\w[^!?.]*[!?.]'\n",
    "tokens1=nltk.regexp_tokenize(text, pattern)\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('`', 'strange'), ('strange', 'days'), ('days', \"'\"), (\"'\", 'chronicles'), ('chronicles', 'the'), ('the', 'last'), ('last', 'two'), ('two', 'days'), ('days', 'of'), ('of', '1999'), ('1999', 'in'), ('in', 'los'), ('los', 'angeles'), ('angeles', '.'), ('.', 'as'), ('as', 'the'), ('the', 'locals'), ('locals', 'gear'), ('gear', 'up'), ('up', 'for'), ('for', 'the'), ('the', 'new'), ('new', 'millenium'), ('millenium', ','), (',', 'lenny'), ('lenny', 'nero'), ('nero', '('), ('(', 'ralph'), ('ralph', 'fiennes'), ('fiennes', ')'), (')', 'goes'), ('goes', 'about'), ('about', 'his'), ('his', 'business'), ('business', 'of'), ('of', 'peddling'), ('peddling', 'erotic'), ('erotic', 'memory'), ('memory', 'clips'), ('clips', '.'), ('.', 'he'), ('he', 'pines'), ('pines', 'for'), ('for', 'his'), ('his', 'ex-girlfriend'), ('ex-girlfriend', ','), (',', 'faith'), ('faith', '('), ('(', 'juliette'), ('juliette', 'lewis'), ('lewis', ')'), (')', ','), (',', 'but'), ('but', 'does'), ('does', \"n't\"), (\"n't\", 'notice'), ('notice', 'that'), ('that', 'another'), ('another', 'friend'), ('friend', ','), (',', 'mace'), ('mace', '('), ('(', 'angela'), ('angela', 'bassett'), ('bassett', ')'), (')', 'really'), ('really', 'cares'), ('cares', 'for'), ('for', 'him'), ('him', '.'), ('.', 'this'), ('this', 'film'), ('film', 'features'), ('features', 'good'), ('good', 'performances'), ('performances', ','), (',', 'impressive'), ('impressive', 'film-making'), ('film-making', 'technique'), ('technique', 'and'), ('and', 'breath-taking'), ('breath-taking', 'crowd'), ('crowd', 'scenes'), ('scenes', '.'), ('.', 'director'), ('director', 'kathryn'), ('kathryn', 'bigelow'), ('bigelow', 'knows'), ('knows', 'her'), ('her', 'stuff'), ('stuff', 'and'), ('and', 'does'), ('does', 'not'), ('not', 'hesitate'), ('hesitate', 'to'), ('to', 'use'), ('use', 'it'), ('it', '.'), ('.', 'but'), ('but', 'as'), ('as', 'a'), ('a', 'whole'), ('whole', ','), (',', 'this'), ('this', 'is'), ('is', 'an'), ('an', 'unsatisfying'), ('unsatisfying', 'movie'), ('movie', '.'), ('.', 'the'), ('the', 'problem'), ('problem', 'is'), ('is', 'that'), ('that', 'the'), ('the', 'writers'), ('writers', ','), (',', 'james'), ('james', 'cameron'), ('cameron', 'and'), ('and', 'jay'), ('jay', 'cocks'), ('cocks', ','), (',', 'were'), ('were', 'too'), ('too', 'ambitious'), ('ambitious', ','), (',', 'aiming'), ('aiming', 'for'), ('for', 'a'), ('a', 'film'), ('film', 'with'), ('with', 'social'), ('social', 'relevance'), ('relevance', ','), (',', 'thrills'), ('thrills', ','), (',', 'and'), ('and', 'drama'), ('drama', '.'), ('.', 'not'), ('not', 'that'), ('that', 'ambitious'), ('ambitious', 'film-making'), ('film-making', 'should'), ('should', 'be'), ('be', 'discouraged'), ('discouraged', ';'), (';', 'just'), ('just', 'that'), ('that', 'when'), ('when', 'it'), ('it', 'fails'), ('fails', 'to'), ('to', 'achieve'), ('achieve', 'its'), ('its', 'goals'), ('goals', ','), (',', 'it'), ('it', 'fails'), ('fails', 'badly'), ('badly', 'and'), ('and', 'obviously'), ('obviously', '.'), ('.', 'the'), ('the', 'film'), ('film', 'just'), ('just', 'ends'), ('ends', 'up'), ('up', 'preachy'), ('preachy', ','), (',', 'unexciting'), ('unexciting', 'and'), ('and', 'uninvolving'), ('uninvolving', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.3.1. Get bigrams from the text                       \n",
    "\n",
    "# bigrams are formed from unigrams\n",
    "# nltk.bigram returns an iterator\n",
    "\n",
    "tokens=nltk.word_tokenize(text)\n",
    "#print(tokens)\n",
    "bigrams=list(nltk.bigrams(tokens))\n",
    "print(bigrams)\n",
    "trigrams=list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.metrics.association.BigramAssocMeasures object at 0x00000165DB5EB080>\n",
      "<nltk.collocations.BigramCollocationFinder object at 0x00000165DB5EB668>\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "# bigram association measures\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "print(bigram_measures)\n",
    "# construct bigrams using words from our example\n",
    "finder = BigramCollocationFinder.from_words(tokens) # tokens are created in Exercise 3.1.4\n",
    "print(finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 'the'),\n",
       " ('it', 'fails'),\n",
       " (\"'\", 'chronicles'),\n",
       " ('(', 'angela'),\n",
       " ('(', 'juliette'),\n",
       " ('(', 'ralph'),\n",
       " (')', ','),\n",
       " (')', 'goes'),\n",
       " (')', 'really'),\n",
       " (',', 'aiming')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the corpus is too small\n",
    "finder.nbest(bigram_measures.raw_freq, 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'and'),\n",
       " (',', '\"'),\n",
       " ('of', 'the'),\n",
       " (\"'\", 's'),\n",
       " ('in', 'the'),\n",
       " ('said', ','),\n",
       " ('said', 'to'),\n",
       " ('.', 'He'),\n",
       " ('the', 'land'),\n",
       " ('.', 'The')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct bigrams using words from a large bulit-in NLTK corpus\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(\\\n",
    "        nltk.corpus.genesis.words('english-web.txt'))\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.collocations.BigramCollocationFinder object at 0x00000165DB725B00>\n"
     ]
    }
   ],
   "source": [
    "print(finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('God', 'said'),\n",
       " ('one', 'hundred'),\n",
       " ('Jacob', 'said'),\n",
       " ('Yahweh', 'God'),\n",
       " ('Yahweh', 'said'),\n",
       " ('years', 'old'),\n",
       " ('seven', 'years'),\n",
       " ('Joseph', 'said'),\n",
       " ('every', 'man'),\n",
       " ('five', 'years')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.2. Find collocation by filter\n",
    "\n",
    "import string\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words\\\n",
    "                         or w.strip(string.punctuation)=='')\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('burnt', 'offering'),\n",
       " ('Paddan', 'Aram'),\n",
       " ('living', 'creature'),\n",
       " ('young', 'lady'),\n",
       " ('little', 'ones'),\n",
       " ('still', 'alive'),\n",
       " ('savory', 'food'),\n",
       " ('creeping', 'thing'),\n",
       " ('find', 'favor'),\n",
       " ('chief', 'cupbearer')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4.1.2 filter bigrams by frequency\n",
    "\n",
    "finder.apply_freq_filter(5)\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dist: <FreqDist with 115 samples and 175 outcomes>\n",
      "top 10 words: [(',', 13), ('.', 9), ('the', 6), ('and', 6), ('for', 4), ('that', 4), ('(', 3), (')', 3), ('film', 3), ('it', 3)]\n",
      "` : 1\n",
      "strange : 1\n",
      "days : 2\n",
      "' : 1\n",
      "chronicles : 1\n",
      "the : 6\n",
      "last : 1\n",
      "two : 1\n",
      "of : 2\n",
      "1999 : 1\n",
      "in : 1\n",
      "los : 1\n",
      "angeles : 1\n",
      ". : 9\n",
      "as : 2\n",
      "locals : 1\n",
      "gear : 1\n",
      "up : 2\n",
      "for : 4\n",
      "new : 1\n",
      "millenium : 1\n",
      ", : 13\n",
      "lenny : 1\n",
      "nero : 1\n",
      "( : 3\n",
      "ralph : 1\n",
      "fiennes : 1\n",
      ") : 3\n",
      "goes : 1\n",
      "about : 1\n",
      "his : 2\n",
      "business : 1\n",
      "peddling : 1\n",
      "erotic : 1\n",
      "memory : 1\n",
      "clips : 1\n",
      "he : 1\n",
      "pines : 1\n",
      "ex-girlfriend : 1\n",
      "faith : 1\n",
      "juliette : 1\n",
      "lewis : 1\n",
      "but : 2\n",
      "does : 2\n",
      "n't : 1\n",
      "notice : 1\n",
      "that : 4\n",
      "another : 1\n",
      "friend : 1\n",
      "mace : 1\n",
      "angela : 1\n",
      "bassett : 1\n",
      "really : 1\n",
      "cares : 1\n",
      "him : 1\n",
      "this : 2\n",
      "film : 3\n",
      "features : 1\n",
      "good : 1\n",
      "performances : 1\n",
      "impressive : 1\n",
      "film-making : 2\n",
      "technique : 1\n",
      "and : 6\n",
      "breath-taking : 1\n",
      "crowd : 1\n",
      "scenes : 1\n",
      "director : 1\n",
      "kathryn : 1\n",
      "bigelow : 1\n",
      "knows : 1\n",
      "her : 1\n",
      "stuff : 1\n",
      "not : 2\n",
      "hesitate : 1\n",
      "to : 2\n",
      "use : 1\n",
      "it : 3\n",
      "a : 2\n",
      "whole : 1\n",
      "is : 2\n",
      "an : 1\n",
      "unsatisfying : 1\n",
      "movie : 1\n",
      "problem : 1\n",
      "writers : 1\n",
      "james : 1\n",
      "cameron : 1\n",
      "jay : 1\n",
      "cocks : 1\n",
      "were : 1\n",
      "too : 1\n",
      "ambitious : 2\n",
      "aiming : 1\n",
      "with : 1\n",
      "social : 1\n",
      "relevance : 1\n",
      "thrills : 1\n",
      "drama : 1\n",
      "should : 1\n",
      "be : 1\n",
      "discouraged : 1\n",
      "; : 1\n",
      "just : 2\n",
      "when : 1\n",
      "fails : 2\n",
      "achieve : 1\n",
      "its : 1\n",
      "goals : 1\n",
      "badly : 1\n",
      "obviously : 1\n",
      "ends : 1\n",
      "preachy : 1\n",
      "unexciting : 1\n",
      "uninvolving : 1\n"
     ]
    }
   ],
   "source": [
    "# 3.5.1 Get token frequency\n",
    "\n",
    "# get unigram frequency \n",
    "# recall, you can also get the dictionary by \n",
    "# {token:count(token) for token in set(tokens)}\n",
    "\n",
    "word_dist=nltk.FreqDist(tokens)\n",
    "print(\"word_dist:\", word_dist)\n",
    "\n",
    "# get the most frequent items\n",
    "print(\"top 10 words:\", word_dist.most_common(10))\n",
    "\n",
    "# what kind of words usually have high frequency?\n",
    "\n",
    "# it behaves as a dictionary\n",
    "for word in word_dist:\n",
    "    print(word,\":\", word_dist[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'film', 'films']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words+=[\"film\", \"films\"]\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5\n"
     ]
    }
   ],
   "source": [
    "y=lambda x:-x\n",
    "print(y(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sort dictionary without stop words by frequency\n",
      "[('days', 2), ('film-making', 2), ('ambitious', 2), ('fails', 2), ('strange', 1), ('chronicles', 1), ('last', 1), ('two', 1), ('1999', 1), ('los', 1), ('angeles', 1), ('locals', 1), ('gear', 1), ('new', 1), ('millenium', 1), ('lenny', 1), ('nero', 1), ('ralph', 1), ('fiennes', 1), ('goes', 1), ('business', 1), ('peddling', 1), ('erotic', 1), ('memory', 1), ('clips', 1), ('pines', 1), ('ex-girlfriend', 1), ('faith', 1), ('juliette', 1), ('lewis', 1), (\"n't\", 1), ('notice', 1), ('another', 1), ('friend', 1), ('mace', 1), ('angela', 1), ('bassett', 1), ('really', 1), ('cares', 1), ('features', 1), ('good', 1), ('performances', 1), ('impressive', 1), ('technique', 1), ('breath-taking', 1), ('crowd', 1), ('scenes', 1), ('director', 1), ('kathryn', 1), ('bigelow', 1), ('knows', 1), ('stuff', 1), ('hesitate', 1), ('use', 1), ('whole', 1), ('unsatisfying', 1), ('movie', 1), ('problem', 1), ('writers', 1), ('james', 1), ('cameron', 1), ('jay', 1), ('cocks', 1), ('aiming', 1), ('social', 1), ('relevance', 1), ('thrills', 1), ('drama', 1), ('discouraged', 1), ('achieve', 1), ('goals', 1), ('badly', 1), ('obviously', 1), ('ends', 1), ('preachy', 1), ('unexciting', 1), ('uninvolving', 1)]\n"
     ]
    }
   ],
   "source": [
    "filtered_dict={word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words and\n",
    "                        word not in string.punctuation}\n",
    "\n",
    "print(\"\\nsort dictionary without stop words by frequency\")\n",
    "#print(filtered_dict.items())\n",
    "#print('\\n')\n",
    "print(sorted(filtered_dict.items(), key=lambda item:-item[1]))\n",
    "#print(sorted(filtered_dict.items()))\n",
    "#print(len(filtered_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['faith', 'good', 'impressive', 'ambitious', 'thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "with open(\"positive-words.txt\",'r') as f:\n",
    "    positive_words=[line.strip() for line in f]\n",
    "\n",
    "#positive_words\n",
    "#print(positive_words)\n",
    "positive_tokens=[token for token in tokens \\\n",
    "                 if token in positive_words]\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['`', 'strange', 'days', \"'\", 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', '.', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', ',', 'lenny', 'nero', '(', 'ralph', 'fiennes', ')', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', '.', 'he', 'pines', 'for', 'his', 'ex-girlfriend', ',', 'faith', '(', 'juliette', 'lewis', ')', ',', 'but', 'does', \"n't\", 'notice', 'that', 'another', 'friend', ',', 'mace', '(', 'angela', 'bassett', ')', 'really', 'cares', 'for', 'him', '.', 'this', 'film', 'features', 'good', 'performances', ',', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', '.', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', '.', 'but', 'as', 'a', 'whole', ',', 'this', 'is', 'an', 'unsatisfying', 'movie', '.', 'the', 'problem', 'is', 'that', 'the', 'writers', ',', 'james', 'cameron', 'and', 'jay', 'cocks', ',', 'were', 'too', 'ambitious', ',', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', ',', 'thrills', ',', 'and', 'drama', '.', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', ';', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', ',', 'it', 'fails', 'badly', 'and', 'obviously', '.', 'the', 'film', 'just', 'ends', 'up', 'preachy', ',', 'unexciting', 'and', 'uninvolving', '.']\n",
      "['faith', 'good', 'impressive', 'thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "# this is not an exhaustive list of negation words!\n",
    "negations=['not', 'too', 'n\\'t', 'no', 'cannot', 'neither','nor']\n",
    "tokens = nltk.word_tokenize(text)  \n",
    "\n",
    "print(tokens)\n",
    "\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>0:\n",
    "            if tokens[idx-1] not in negations:\n",
    "                positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['faith', 'good', 'impressive', 'ambitious', 'thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "N=3;\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>N-1:\n",
    "            for n in range(1,N+1):\n",
    "                if (tokens[idx-n] not in negations):\n",
    "                    if n==N:\n",
    "                        positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)\n",
    "# why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Q1\n"
     ]
    }
   ],
   "source": [
    "text='''contact Yahoo! at \"http://login.yahoo.com\", select forgot\n",
    "your password. If that fails to reset, contact Yahoo! at\n",
    "their password department 408-349-1572 -- Can't promise\n",
    "their phone department will fix, but they'll know where to\n",
    "go next. Corporate emails from Yahoo! don't come from\n",
    "their free mail system address space. Webmaster@yahoo.com\n",
    "is not a corporate email address.'''\n",
    "print(\"Test Q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "['contact', 'yahoo', 'at', 'http://login.yahoo.com', 'select', 'forgot', 'your', 'password', 'if', 'that', 'fails', 'to', 'reset', 'contact', 'yahoo', 'at', 'their', 'password', 'department', '408', '349', '1572', 'can', 'promise', 'their', 'phone', 'department', 'will', 'fix', 'but', 'they', 'll', 'know', 'where', 'to', 'go', 'next', 'corporate', 'emails', 'from', 'yahoo', 'don', 'come', 'from', 'their', 'free', 'mail', 'system', 'address', 'space', 'webmaster@yahoo.com', 'is', 'not', 'corporate', 'email', 'address']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a797ece24fdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mlemmtokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[1;31m# 0. Check the exception lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "#convert the string to lowercase\n",
    "text.lower()\n",
    "#Each token has at least two characters.\n",
    "#The first/last character can only be a letter (i.e. a-z) or a number (0-9)\n",
    "#In the middle, there are 0 or more characters, which can only be letters (a-z),\n",
    "#numbers (0-9), hyphens (\"-\"), underscores (\"_\"), dot (\".\"), or \"@\" symbols.\n",
    "\n",
    "pattern=r'\\w+[\\w\\.-_@]*\\w+'   \n",
    "tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "print(len(tokens))\n",
    "print (tokens)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for token in tokens:\n",
    "    lemmtokens=lemmatizer.lemmatize(tokens)\n",
    "print(lemmtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    token_count = None\n",
    "    # add your code here\n",
    "    \n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "# Test Q1\n",
    "text='''contact Yahoo! at \"http://login.yahoo.com\", select forgot\n",
    "your password. If that fails to reset, contact Yahoo! at\n",
    "their password department 408-349-1572 -- Can't promise\n",
    "their phone department will fix, but they'll know where to\n",
    "go next. Corporate emails from Yahoo! don't come from\n",
    "their free mail system address space. Webmaster@yahoo.com\n",
    "is not a corporate email address.'''\n",
    "print(\"Test Q1\")\n",
    "for key, value in tokenize(text).items():\n",
    "    print(key, value)\n",
    "# You should get the result look like :\n",
    "# contact 2 yahoo 3 http 1 login.yahoo.com 1\n",
    "# select 1 forget 1 password 2 fail 1\n",
    "# reset 1 department 2 408-349-1572 1 promise 1\n",
    "# phone 1 fix 1 know 1 go 1\n",
    "# next 1 corporate 2 email 2 come 1\n",
    "# free 1 mail 1 system 1 address 2\n",
    "# space 1 webmaster@yahoo.com 1\n",
    "data=pd.read_csv(\"qa.csv\", header=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
